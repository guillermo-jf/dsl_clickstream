{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988873e3-12ff-4e4e-b11f-ba7d0433319a",
   "metadata": {},
   "source": [
    "## Task 2: Writing Dataflow batch pipelines\n",
    "\n",
    "In this task, you use Apache Beam and Dataflow to run a batch processing pipeline to accomplish the same job as in the previous task. Read the data from Cloud Storage, parse it, and write it to BigQuery using a schema that is optimized for analytics.\n",
    "\n",
    "Using Apache Beam, create a pipeline to migrate the clickstream data to BigQuery in accordance with the schema you created earlier. Program the pipeline in a Jupyter Notebook.\n",
    "\n",
    "Once you have the pipeline tested, run it using Google Cloud Dataflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2becb0-8fbf-43bf-9639-3c4288449c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install --upgrade apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cbf13-4a44-4d55-9f19-0816fb9241a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9824019b-be80-422e-81cf-b639d6877e63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your Google Cloud project ID and dataset\n",
    "PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "DATASET_ID = \"dsl_project\"\n",
    "TABLE_ID = \"website-visits\"\n",
    "TEMP_LOCATION = f\"gs://{PROJECT_ID}/temp\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Create the unique job name by appending the timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_NAME = f\"storage-to-bq-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc6103-25f6-4478-bc82-c1c03788c4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14764e9c-b0fd-427e-ba04-0258fdd67f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify your GCS JSONL file path\n",
    "input_path = \"gs://guillermo-lake/visits-*.jsonl\"\n",
    "\n",
    "# Define the BigQuery schema as a JSON object (not a plain string)\n",
    "SCHEMA = {\n",
    "    \"fields\": [\n",
    "        {\"name\": \"session_id\", \"type\": \"STRING\"},\n",
    "        {\"name\": \"user_id\", \"type\": \"STRING\"},\n",
    "        {\"name\": \"device_type\", \"type\": \"STRING\"},\n",
    "        {\"name\": \"geolocation\", \"type\": \"STRING\"},\n",
    "        {\"name\": \"user_agent\", \"type\": \"STRING\"},\n",
    "        {\n",
    "            \"name\": \"events\",\n",
    "            \"type\": \"RECORD\",\n",
    "            \"mode\": \"REPEATED\",\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"event\",\n",
    "                    \"type\": \"RECORD\",\n",
    "                    \"fields\": [\n",
    "                        {\"name\": \"event_type\", \"type\": \"STRING\"},\n",
    "                        {\"name\": \"timestamp\", \"type\": \"TIMESTAMP\"},\n",
    "                        {\n",
    "                            \"name\": \"details\",\n",
    "                            \"type\": \"RECORD\",\n",
    "                            \"fields\": [\n",
    "                                {\"name\": \"page_url\", \"type\": \"STRING\"},\n",
    "                                {\"name\": \"referrer_url\", \"type\": \"STRING\"},\n",
    "                                {\"name\": \"product_id\", \"type\": \"STRING\"},\n",
    "                                {\"name\": \"product_name\", \"type\": \"STRING\"},\n",
    "                                {\"name\": \"category\", \"type\": \"STRING\"},\n",
    "                                {\"name\": \"price\", \"type\": \"FLOAT\"},\n",
    "                                {\"name\": \"quantity\", \"type\": \"INTEGER\"},\n",
    "                                {\"name\": \"order_id\", \"type\": \"STRING\"},\n",
    "                                {\"name\": \"amount\", \"type\": \"FLOAT\"},\n",
    "                                {\"name\": \"currency\", \"type\": \"STRING\"},\n",
    "                                {\n",
    "                                    \"name\": \"items\",\n",
    "                                    \"type\": \"RECORD\",\n",
    "                                    \"mode\": \"REPEATED\",\n",
    "                                    \"fields\": [\n",
    "                                        {\"name\": \"product_id\", \"type\": \"STRING\"},\n",
    "                                        {\"name\": \"product_name\", \"type\": \"STRING\"},\n",
    "                                        {\"name\": \"category\", \"type\": \"STRING\"},\n",
    "                                        {\"name\": \"price\", \"type\": \"FLOAT\"},\n",
    "                                        {\"name\": \"quantity\", \"type\": \"INTEGER\"},\n",
    "                                    ],\n",
    "                                },\n",
    "                            ],\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "class ParseJsonlFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        try:\n",
    "            record = json.loads(element)\n",
    "            yield record\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing record: {e}\")\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "def create_table():\n",
    "    \"\"\"Creates the BigQuery table if it does not exist.\"\"\"\n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    table_ref = dataset_ref.table(TABLE_ID)\n",
    "    table = bigquery.Table(table_ref, schema=[bigquery.SchemaField.from_api_repr(field) for field in SCHEMA[\"fields\"]])\n",
    "    \n",
    "    try:\n",
    "        client.create_table(table)\n",
    "        print(f\"Table {TABLE_ID} created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Table creation failed: {e}\")\n",
    "\n",
    "def run_pipeline(input_path):\n",
    "    \"\"\"Runs the Apache Beam pipeline to process JSONL files.\"\"\"\n",
    "    options = PipelineOptions(flags=[\n",
    "        \"--project\", PROJECT_ID,\n",
    "        \"--runner\", \"DirectRunner\",  # Runs locally\n",
    "        \"--temp_location\", f\"gs://{PROJECT_ID}/temp\"\n",
    "    ])\n",
    "\n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | \"Read JSONL File\" >> beam.io.ReadFromText(input_path)\n",
    "            | \"Parse JSON\" >> beam.ParDo(ParseJsonlFn())\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                table=f\"{PROJECT_ID}:{DATASET_ID}.{TABLE_ID}\",\n",
    "                schema=SCHEMA,  # Pass the schema directly as a Python dictionary\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                custom_gcs_temp_location=f\"gs://{PROJECT_ID}/temp\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Run table creation\n",
    "#create_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d732d-4a8d-4e75-9387-4e3be7b1bae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run the pipeline locally\n",
    "run_pipeline(input_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ecc46-a7c9-44ae-90af-c29cc6901218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['-f', '/home/jupyter/.local/share/jupyter/runtime/kernel-0c430f67-bf4d-4c04-b8b8-d8b0a275dca9.json']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    div.alert {\n",
       "      white-space: pre-line;\n",
       "    }\n",
       "  </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://jellyfish-training-demo-6/staging for cache location.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.UpdateDestinationSchema'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.TriggerCopyJobs'>)\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class 'apache_beam.io.gcp.bigquery_file_loads.DeleteTablesFn'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: -f,/home/jupyter/.local/share/jupyter/runtime/kernel-0c430f67-bf4d-4c04-b8b8-d8b0a275dca9.json. Ignore if flags are used for internal purposes.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: -f,/home/jupyter/.local/share/jupyter/runtime/kernel-0c430f67-bf4d-4c04-b8b8-d8b0a275dca9.json. Ignore if flags are used for internal purposes.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n"
     ]
    }
   ],
   "source": [
    "# Define and run the pipeline options for dataflow\n",
    "\n",
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = PROJECT_ID\n",
    "google_cloud_options.job_name = JOB_NAME\n",
    "google_cloud_options.staging_location = f'gs://{PROJECT_ID}/staging'\n",
    "google_cloud_options.temp_location = f'gs://{PROJECT_ID}/temp'\n",
    "google_cloud_options.region = REGION  \n",
    "options.view_as(beam.options.pipeline_options.StandardOptions).runner = 'DataflowRunner'\n",
    "\n",
    "\n",
    "with beam.Pipeline(options=options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | \"Read JSONL File\" >> beam.io.ReadFromText(input_path)\n",
    "            | \"Parse JSON\" >> beam.ParDo(ParseJsonlFn())\n",
    "            | \"Write to BigQuery\" >> beam.io.WriteToBigQuery(\n",
    "                f\"{PROJECT_ID}:{DATASET_ID}.{TABLE_ID}\",\n",
    "                schema=SCHEMA,  # Pass the schema directly as a Python dictionary\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "\n",
    "            )\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceca6a5-94c8-42b7-9a67-1065440790b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
