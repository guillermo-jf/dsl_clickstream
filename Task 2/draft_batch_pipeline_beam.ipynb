{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a4daae-ec56-450d-8c70-fdd92f3f00a2",
   "metadata": {},
   "source": [
    "## Task 2: Writing Dataflow batch pipelines\n",
    "\n",
    "In this task, you use Apache Beam and Dataflow to run a batch processing pipeline to accomplish the same job as in the previous task. Read the data from Cloud Storage, parse it, and write it to BigQuery using a schema that is optimized for analytics.\n",
    "\n",
    "Using Apache Beam, create a pipeline to migrate the clickstream data to BigQuery in accordance with the schema you created earlier. Program the pipeline in a Jupyter Notebook.\n",
    "\n",
    "Once you have the pipeline tested, run it using Google Cloud Dataflow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489209b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.io.gcp.bigquery import BigQueryDisposition\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762089ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parse_visit function\n",
    "# import sys\n",
    "# lib_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dsllib'))\n",
    "# if lib_path not in sys.path:\n",
    "#     print(f\"Appending path {lib_path}\")\n",
    "#     sys.path.append(lib_path)\n",
    "\n",
    "# from dsllib.visits import parse_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "credentials, project = google.auth.default()\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d45292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "PROJECT_ID=os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
    "REGION=os.environ.get(\"REGION\")\n",
    "BUCKET=os.environ.get(\"BUCKET\")\n",
    "DATASET=os.environ.get(\"DATASET\")\n",
    "TABLE=os.environ.get(\"TABLE\")\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ea5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_visit(element: str):\n",
    "    \"\"\"\n",
    "    Parses a JSON string representing a user visit and extracts relevant information.\n",
    "\n",
    "    Args:\n",
    "        element (str): A JSON string containing visit data.\n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed visit data, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    import json\n",
    "    from datetime import datetime\n",
    "\n",
    "    try:\n",
    "        visit_data = json.loads(element)\n",
    "\n",
    "        session_id = visit_data.get(\"session_id\")\n",
    "        user_id = visit_data.get(\"user_id\")\n",
    "        device_type = visit_data.get(\"device_type\")\n",
    "        \n",
    "        #Parse geolocation data\n",
    "        geo_str = visit_data.get(\"geolocation\")\n",
    "        lat, lon = geo_str.split(',')\n",
    "        geolocation = f\"POINT({lon} {lat})\"  # Convert to WKT\n",
    "\n",
    "        user_agent = visit_data.get(\"user_agent\")\n",
    "\n",
    "        events = visit_data.get(\"events\", [])\n",
    "        \n",
    "        visit_start_time = None\n",
    "        visit_end_time = None\n",
    "        \n",
    "        formatted_events = []\n",
    "        for event_data in events:\n",
    "            event = event_data.get(\"event\", {})\n",
    "            event_type = event.get(\"event_type\")\n",
    "            timestamp_str = event.get(\"timestamp\")\n",
    "            timestamp = datetime.fromisoformat(timestamp_str)\n",
    "            \n",
    "            if visit_start_time is None or timestamp < visit_start_time:\n",
    "                visit_start_time = timestamp\n",
    "            if visit_end_time is None or timestamp > visit_end_time:\n",
    "                visit_end_time = timestamp\n",
    "\n",
    "            details = event.get(\"details\", {})\n",
    "            page_view_details = {}\n",
    "            add_cart_details = {}\n",
    "            purchase_details = {}\n",
    "\n",
    "            if event_type == \"page_view\":\n",
    "                page_view_details = {\n",
    "                    \"page_url\": details.get(\"page_url\"),\n",
    "                    \"referrer_url\": details.get(\"referrer_url\"),\n",
    "                }\n",
    "            elif event_type == \"add_item_to_cart\":\n",
    "                add_cart_details = {\n",
    "                    \"product_id\": details.get(\"product_id\"),\n",
    "                    \"product_name\": details.get(\"product_name\"),\n",
    "                    \"category\": details.get(\"category\"),\n",
    "                    \"price\": details.get(\"price\"),\n",
    "                    \"quantity\": details.get(\"quantity\"),\n",
    "                }\n",
    "            elif event_type == \"purchase\":\n",
    "                purchase_details = {\n",
    "                    \"order_id\": details.get(\"order_id\"),\n",
    "                    \"amount\": details.get(\"amount\"),\n",
    "                    \"currency\": details.get(\"currency\"),\n",
    "                    \"items\": details.get(\"items\"),\n",
    "                }\n",
    "\n",
    "            formatted_events.append(\n",
    "                {\n",
    "                    \"event_type\": event_type,\n",
    "                    \"event_timestamp\": timestamp.isoformat(),\n",
    "                    \"page_view\": page_view_details,\n",
    "                    \"add_cart\": add_cart_details,\n",
    "                    \"purchase\": purchase_details,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        row = {\n",
    "            \"session_id\": session_id,\n",
    "            \"user_id\": user_id,\n",
    "            \"device_type\": device_type,\n",
    "            \"geolocation\": geolocation,\n",
    "            \"user_agent\": user_agent,\n",
    "            \"visit_start_time\": visit_start_time.isoformat() if visit_start_time else None,\n",
    "            \"visit_end_time\": visit_end_time.isoformat() if visit_end_time else None,\n",
    "            \"events\": formatted_events,\n",
    "        }\n",
    "        \n",
    "        return (row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing element: {e}\")\n",
    "        print(f\"Problematic element: {element}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62402682",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts=datetime.now().isoformat()\n",
    "\n",
    "def run_pipeline(input_path, output_table, runner, project_id, region):\n",
    "    #ts=datetime.now().isoformat()\n",
    "    ts=datetime.now().strftime(\"%Y-%m-%dt%H%M%S\")\n",
    "\n",
    "    # pipe_opts = {\n",
    "    #     'runner': runner,\n",
    "    #     'project': project_id,\n",
    "    #     'job_name': f\"load-events-pipeline-{ts}\",\n",
    "    #     'save_main_session': True,\n",
    "    #     'region': region,\n",
    "    #     'temp_location': f\"gs://{BUCKET}/tmp-{ts}\",\n",
    "    #     'staging_location': f\"gs://{BUCKET}/staging-{ts}\",\n",
    "    # }\n",
    "\n",
    "    # if runner is 'DataflowRunner':\n",
    "    #     df_opts = {\n",
    "    #         'region': region,\n",
    "    #         'temp_location': f\"gs://{BUCKET}/tmp-{ts}\",\n",
    "    #         'staging_location': f\"gs://{BUCKET}/staging-{ts}\",\n",
    "    #     }\n",
    "    #     pipe_opts.update(df_opts)\n",
    "\n",
    "\n",
    "    #pipeline_options = PipelineOptions(flags=[], **pipe_opts)\n",
    "\n",
    "    beam_options = PipelineOptions(\n",
    "        flags=[],\n",
    "        runner=runner,\n",
    "        project=project_id,\n",
    "        region=region,\n",
    "        temp_location=f\"gs://{BUCKET}/tmp-{ts}/\",\n",
    "        staging_location=f\"gs://{BUCKET}/staging-{ts}\",\n",
    "        job_name=f\"load-events-pipeline-{ts}\",\n",
    "        #save_main_session=True\n",
    "    )\n",
    "\n",
    "    with beam.Pipeline(options=beam_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | \"ReadFromGCS\" >> beam.io.ReadFromText(input_path) # type: ignore\n",
    "            | \"ParseVisit\" >> beam.Map(parse_visit)\n",
    "            | \"WriteToBQ\" >> beam.io.WriteToBigQuery(\n",
    "                table=output_table,\n",
    "                #table=f\"{TABLE}_test\",\n",
    "                #dataset=DATASET,\n",
    "                #project=PROJECT_ID,\n",
    "                custom_gcs_temp_location=f\"gs://{BUCKET}/bqtmp-{ts}\",\n",
    "                #This table should be created ahead of time. If it doesn't exist, don't create it.\n",
    "                create_disposition=BigQueryDisposition.CREATE_NEVER,\n",
    "                #This pipeline only appends data. Don't overwrite.\n",
    "                write_disposition=BigQueryDisposition.WRITE_APPEND,\n",
    "                #method=beam.io.WriteToBigQuery.Method.STREAMING_INSERTS\n",
    "                #method=beam.io.WriteToBigQuery.Method.STORAGE_WRITE_API\n",
    "            )\n",
    "            # | \"WriteToBigQuery\" >> beam.io.WriteToBigQuery(\n",
    "            #     output_table,\n",
    "            #     schema=bq_schema,\n",
    "            #     #This table should be created ahead of time. If it doesn't exist, don't create it.\n",
    "            #     create_disposition=BigQueryDisposition.CREATE_NEVER,\n",
    "            #     #This pipeline only appends data. Don't overwrite.\n",
    "            #     write_disposition=BigQueryDisposition.WRITE_APPEND,\n",
    "            #     method=beam.io.WriteToBigQuery.Method.STREAMING_INSERTS\n",
    "            #     #method=beam.io.WriteToBigQuery.Method.FILE_LOADS\n",
    "            #     )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a9e5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = beam.Pipeline(runner='InteractiveRunner')\n",
    "text_coll = pipe | \"ReadFromLocal\" >> beam.io.ReadFromText(\"../challenge-clickstream/data/visits-2024-07-01.jsonl\")\n",
    "json_coll = text_coll | \"ParseVisit\" >> beam.Map(parse_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9085fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_out_coll = json_coll | \"WriteToFile\" >> beam.io.WriteToText(\"out/testme\")\n",
    "#pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d556b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts=datetime.now().isoformat()\n",
    "ts=datetime.now().strftime(\"%Y-%m-%dT%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_table = f\"{PROJECT_ID}:{DATASET}.{TABLE}\"\n",
    "output_table = f\"{DATASET}.{TABLE}_test\"\n",
    "\n",
    "print(f\"Output table: {output_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_result = json_coll | \"WriteToBQ\" >> beam.io.WriteToBigQuery(\n",
    "                table=f\"{TABLE}_test\",\n",
    "                dataset=DATASET,\n",
    "                project=PROJECT_ID,\n",
    "                #schema=bq_schema,\n",
    "                custom_gcs_temp_location=f\"gs://{BUCKET}/tmp-{ts}\",\n",
    "                #This table should be created ahead of time. If it doesn't exist, don't create it.\n",
    "                create_disposition=BigQueryDisposition.CREATE_NEVER,\n",
    "                #This pipeline only appends data. Don't overwrite.\n",
    "                write_disposition=BigQueryDisposition.WRITE_APPEND,\n",
    "                #method=beam.io.WriteToBigQuery.Method.STREAMING_INSERTS\n",
    "                #method=beam.io.WriteToBigQuery.Method.STORAGE_WRITE_API\n",
    "                )\n",
    "\n",
    "pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe.run()\n",
    "\n",
    "# vals = ib.collect(json_coll)\n",
    "# vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f\"gs://jfdemo3-dsl/data/visits-*.jsonl\"\n",
    "output_table = f\"{DATASET}.{TABLE}\"\n",
    "project_id = PROJECT_ID\n",
    "ts=datetime.now().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "#runner=\"DirectRunner\"\n",
    "runner=\"DataflowRunner\"\n",
    "run_pipeline(input_path, output_table, runner, project_id, region=REGION)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
