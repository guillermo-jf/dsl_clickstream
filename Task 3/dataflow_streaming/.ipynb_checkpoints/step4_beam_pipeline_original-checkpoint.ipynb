{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9552900-bbd0-44aa-b7a6-aa93ce44d1be",
   "metadata": {},
   "source": [
    "Write an Apache Beam pipeline with the following requirements:\n",
    "\n",
    "Write the raw data to files in Google Cloud Storage at regular intervals.\n",
    "Parse the messages and write the data to BigQuery.\n",
    "Calculate page views by minute. Create a dashboard that reports this information.\n",
    "Run the pipeline in Dataflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda21111-ad36-4335-b20c-a580c9a2b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gcloud auth application-default login\n",
    "#gcloud config set project your-gcp-project-id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123cb65-18b0-49cf-912b-dac35b8abdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet \"apache-beam[gcp]\" \"google-cloud-pubsub\" \"notebook\"\n",
    "!pip install --upgrade apache-beam[gcp]\n",
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc9623-40b6-40c1-a629-d50b99462788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pubsub subscription to be used for the beam pipeline\n",
    "gcloud pubsub subscriptions create dsl-clickstream-push-beam \\\n",
    "    --topic=dsl-project-clickstream\n",
    "\n",
    "# this is a pull subscription, even though we are building a streaming pipeline\n",
    "# dataflow will handle the processing of data as quickly as possible, using the proper IO pubsub components within Beam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae63066-08ca-46f9-85ba-6f5b5f91c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, DirectOptions\n",
    "import logging\n",
    "import time\n",
    "from apache_beam.io import fileio\n",
    "from apache_beam.transforms import window\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78f2f6-c557-484f-bf8c-01f5cd641280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CORRECTION APPLIED HERE ---\n",
    "# The error indicated that the runner's internal DoFn calls sink.write(record),\n",
    "# not sink.write(record, file_handle). This fix adapts the sink to that\n",
    "# calling convention by storing the file handle from open() as an instance\n",
    "# variable and using it in the write() and flush() methods.\n",
    "class JsonlSink(fileio.FileSink):\n",
    "    \"\"\"A sink that writes JSONL records to a file.\"\"\"\n",
    "    def __init__(self):\n",
    "        # The sink needs a coder to encode the strings into bytes.\n",
    "        self._coder = beam.coders.StrUtf8Coder()\n",
    "        self._fh = None\n",
    "\n",
    "    def open(self, fh):\n",
    "        \"\"\"This method is called when a new file is created for writing.\"\"\"\n",
    "        self._fh = fh\n",
    "\n",
    "    def write(self, record):\n",
    "        \"\"\"\n",
    "        This method is called for each element. It uses the file handle\n",
    "        that was saved by the open() method.\n",
    "        \"\"\"\n",
    "        if self._fh is None:\n",
    "            raise RuntimeError('Sink open() was not called before write()')\n",
    "        self._fh.write(self._coder.encode(record) + b'\\n')\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"\n",
    "        This method is called before a file is closed. It uses the file handle\n",
    "        that was saved by the open() method.\n",
    "        \"\"\"\n",
    "        if self._fh is not None:\n",
    "            self._fh.flush()\n",
    "\n",
    "def define_and_run_pipeline(pipeline_options, subscription_path, output_path):\n",
    "    \"\"\"\n",
    "    Defines and runs the streaming Beam pipeline.\n",
    "    \"\"\"\n",
    "    # --- FIX APPLIED HERE ---\n",
    "    # The \"unimplemented in prism\" error indicates the default DirectRunner (Prism)\n",
    "    # doesn't support the Pub/Sub IO transform being used.\n",
    "    # This line forces the runner to use the older, multi-threading execution\n",
    "    # mode, which is more broadly compatible.\n",
    "    pipeline_options.view_as(DirectOptions).direct_running_mode = 'multi_threading'\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(\n",
    "                subscription=subscription_path,\n",
    "              ).with_output_types(bytes)\n",
    "            \n",
    "            | 'Decode Messages' >> beam.Map(lambda msg_bytes: msg_bytes.decode('utf-8'))\n",
    "            | 'Parse JSON' >> beam.Map(json.loads)\n",
    "            # Use a DoFn for logging to avoid returning None, which is good practice.\n",
    "            | 'Log Payloads' >> beam.Map(lambda data: logging.info(f\"Processing session: {data.get('session_id')}\") or data)\n",
    "            | 'Convert to JSONL' >> beam.Map(json.dumps)\n",
    "\n",
    "            # --- CHANGE APPLIED HERE ---\n",
    "            # Apply a 60-second (1 minute) fixed window. Dataflow will buffer\n",
    "            # messages for one minute before passing them to the write stage.\n",
    "            | 'Window into 1 Minute Batches' >> beam.WindowInto(window.FixedWindows(60))\n",
    "\n",
    "            # --- SOLID SOLUTION APPLIED HERE ---\n",
    "            # Replaced WriteToText with the more robust WriteToFiles transform.\n",
    "            # This transform is designed for windowed, streaming writes and avoids\n",
    "            # the internal GroupByKey error seen with the DirectRunner by writing\n",
    "            # each window's data to a separate file.\n",
    "            | 'Write Windowed Files' >> fileio.WriteToFiles(\n",
    "                path=output_path, # This should be a directory, e.g., \"gs://bucket/path/\"\n",
    "                sink=JsonlSink(),\n",
    "                file_naming=fileio.default_file_naming(prefix=\"visits-\", suffix=\".jsonl\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    logging.info(\"Pipeline defined. Runner will now execute it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f2bd4-5442-425f-883e-3ea5dc6088a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2: Execute Locally with DirectRunner\n",
    "\n",
    "import logging\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "# --- Variables ---\n",
    "PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "SUBSCRIPTION_NAME = \"dsl-clickstream-push-beam\"\n",
    "\n",
    "# Correctly separate the GCS bucket from the path/prefix\n",
    "GCS_BUCKET = \"jellyfish-training-demo-6\" \n",
    "GCS_PATH = \"dsl-project\" # Path inside the bucket\n",
    "\n",
    "# Let's write to a specific 'test' folder to keep things organized\n",
    "LOCAL_TEST_OUTPUT_PREFIX = f\"gs://{GCS_BUCKET}/{GCS_PATH}/local_run/clickstream\"\n",
    "\n",
    "# --- 2. Create Pipeline Options ---\n",
    "# Corrected spacing and ensured no invalid characters\n",
    "local_options = PipelineOptions(\n",
    "    streaming=True,\n",
    "    project=PROJECT_ID\n",
    "    # No runner specified, so it defaults to DirectRunner\n",
    ")\n",
    "\n",
    "# --- 3. Run the Pipeline ---\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "print(\"--- Starting pipeline locally using DirectRunner ---\")\n",
    "print(f\"Reading from: projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\")\n",
    "print(f\"Writing to: {LOCAL_TEST_OUTPUT_PREFIX}*\")\n",
    "print(\"\\nPipeline is running. Publish a message to the topic to see output.\")\n",
    "print(\"To STOP, you MUST interrupt the Jupyter kernel (Kernel -> Interrupt).\")\n",
    "\n",
    "# Call the function defined in the Canvas\n",
    "# Corrected spacing and ensured no invalid characters\n",
    "define_and_run_pipeline(\n",
    "    pipeline_options=local_options,\n",
    "    subscription_path=f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\",\n",
    "    output_path=LOCAL_TEST_OUTPUT_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d8d30-59d4-4d5b-99cf-56abced3a917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: To Run the Pipeline on Dataflow\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions\n",
    "\n",
    "# --- 1. Set up your project and GCS details ---\n",
    "PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "SUBSCRIPTION_NAME = \"dsl-clickstream-push-beam\"\n",
    "GCS_BUCKET = \"jellyfish-training-demo-6\"\n",
    "GCS_PATH = \"dsl-project\" # Path inside the bucket\n",
    "REGION = 'us-central1'   # Make sure this is a valid region\n",
    "\n",
    "# --- 2. Configure Dataflow-specific options ---\n",
    "# It's a good practice to make the job name unique by adding a timestamp.\n",
    "JOB_NAME = f'streaming-clickstream-{int(time.time())}'\n",
    "TEMP_LOCATION = f\"gs://{GCS_BUCKET}/{GCS_PATH}/temp\"\n",
    "DATAFLOW_OUTPUT_PREFIX = f\"gs://{GCS_BUCKET}/{GCS_PATH}/dataflow_run/\"\n",
    "\n",
    "# --- 3. Create and configure pipeline options for Dataflow ---\n",
    "# Using the structure you requested.\n",
    "options = PipelineOptions()\n",
    "\n",
    "# Set Google Cloud and Dataflow specific options\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = PROJECT_ID\n",
    "google_cloud_options.job_name = JOB_NAME\n",
    "google_cloud_options.staging_location = f\"gs://{GCS_BUCKET}/{GCS_PATH}/staging\"\n",
    "google_cloud_options.temp_location = TEMP_LOCATION\n",
    "google_cloud_options.region = REGION\n",
    "\n",
    "# Set the runner\n",
    "options.view_as(StandardOptions).runner = 'DataflowRunner'\n",
    "# For streaming jobs\n",
    "options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "# Set the setup file for the custom sink dependency\n",
    "options.view_as(SetupOptions).setup_file = './setup.py'\n",
    "\n",
    "\n",
    "# --- 4. Run the pipeline on Dataflow ---\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "print(f\"--- Starting Dataflow Job: {JOB_NAME} ---\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Writing to: {DATAFLOW_OUTPUT_PREFIX}*\")\n",
    "print(\"\\nYou can monitor the job's progress in the Google Cloud Console.\")\n",
    "\n",
    "define_and_run_pipeline(\n",
    "    pipeline_options=options,\n",
    "    subscription_path=f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\",\n",
    "    output_path=DATAFLOW_OUTPUT_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8052f0-23bb-454c-8c1e-de284527aef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2: Securely View Pipeline Output\n",
    "# Run this cell a few minutes after the pipeline has started.\n",
    "\n",
    "\n",
    "\n",
    "# --- You may need to install the library first ---\n",
    "# !pip install google-cloud-storage\n",
    "\n",
    "# --- Configuration (should match your pipeline) ---\n",
    "GCS_BUCKET = \"jellyfish-training-demo-6\" \n",
    "GCS_PATH = \"dsl-project/dataflow_run\" # The output path for your data\n",
    "\n",
    "# --- Use the Client Library to access GCS ---\n",
    "# This uses your gcloud credentials automatically.\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCS_BUCKET)\n",
    "\n",
    "# List all files in the output directory\n",
    "blobs = list(bucket.list_blobs(prefix=GCS_PATH))\n",
    "\n",
    "if not blobs:\n",
    "    print(\"No output files found yet. Please wait a few more minutes.\")\n",
    "    print(f\"Searched in: gs://{GCS_BUCKET}/{GCS_PATH}\")\n",
    "else:\n",
    "    # Find the most recently created file\n",
    "    latest_blob = sorted(blobs, key=lambda b: b.time_created, reverse=True)[0]\n",
    "    \n",
    "    print(f\"--- Displaying contents of the latest output file ---\")\n",
    "    print(f\"File: gs://{GCS_BUCKET}/{latest_blob.name}\")\n",
    "    print(f\"Created at: {latest_blob.time_created}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Download and print the content\n",
    "    content = latest_blob.download_as_text()\n",
    "    \n",
    "    # Print each JSON line\n",
    "    for line in content.strip().split('\\n'):\n",
    "        # Pretty-print the JSON for readability\n",
    "        parsed_json = json.loads(line)\n",
    "        print(json.dumps(parsed_json, indent=2))\n",
    "        \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae2afd-6d99-4139-a8da-da1de30c6b23",
   "metadata": {},
   "source": [
    "## Updating the pipeline to include a function to populate jsonl data to BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d89f95-d812-4a97-9761-9ff63303bfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Python Script to Launch Dataflow Job ---\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from datetime import datetime\n",
    "from apache_beam.io import fileio\n",
    "from apache_beam.transforms import window\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions\n",
    "\n",
    "# --- BIGQUERY TABLE SCHEMA ---\n",
    "TABLE_SCHEMA = {\n",
    "    'fields': [\n",
    "        {'name': 'session_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'user_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'device_type', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'geolocation', 'type': 'GEOGRAPHY', 'mode': 'NULLABLE'},\n",
    "        {'name': 'user_agent', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'visit_start_time', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "        {'name': 'visit_end_time', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "        {\n",
    "            'name': 'events', 'type': 'RECORD', 'mode': 'REPEATED',\n",
    "            'fields': [\n",
    "                {'name': 'event_type', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                {'name': 'event_timestamp', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "                {\n",
    "                    'name': 'page_view', 'type': 'RECORD', 'mode': 'NULLABLE',\n",
    "                    'fields': [\n",
    "                        {'name': 'page_url', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'referrer_url', 'type': 'STRING', 'mode': 'NULLABLE'}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    'name': 'add_cart', 'type': 'RECORD', 'mode': 'NULLABLE',\n",
    "                    'fields': [\n",
    "                        {'name': 'product_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'product_name', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'category', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'price', 'type': 'FLOAT64', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'quantity', 'type': 'INT64', 'mode': 'NULLABLE'}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    'name': 'purchase', 'type': 'RECORD', 'mode': 'NULLABLE',\n",
    "                    'fields': [\n",
    "                        {'name': 'order_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'amount', 'type': 'FLOAT64', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'currency', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {\n",
    "                            'name': 'items', 'type': 'RECORD', 'mode': 'REPEATED',\n",
    "                            'fields': [\n",
    "                                {'name': 'product_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'product_name', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'category', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'price', 'type': 'FLOAT64', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'quantity', 'type': 'INT64', 'mode': 'NULLABLE'}\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75877175-e2eb-4638-b639-6a89a10fdbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CUSTOM SINK FOR WRITING JSONL FILES TO GCS ---\n",
    "class JsonlSink(fileio.FileSink):\n",
    "    def __init__(self):\n",
    "        self._coder = beam.coders.StrUtf8Coder()\n",
    "        self._fh = None\n",
    "    def open(self, fh):\n",
    "        self._fh = fh\n",
    "    def write(self, record):\n",
    "        if self._fh is None:\n",
    "            raise RuntimeError('Sink open() was not called before write()')\n",
    "        self._fh.write(self._coder.encode(record) + b'\\n')\n",
    "    def flush(self):\n",
    "        if self._fh is not None:\n",
    "            self._fh.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac489445-c462-4cf6-b2a6-3774a4c867b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CUSTOM TRANSFORMATION LOGIC FOR BIGQUERY ---\n",
    "def transform_json_to_bigquery_row(message_body: str) -> dict:\n",
    "    data = json.loads(message_body)\n",
    "    geolocation_wkt = None\n",
    "    if 'geolocation' in data and data['geolocation']:\n",
    "        try:\n",
    "            lat, lon = map(float, data['geolocation'].split(','))\n",
    "            geolocation_wkt = f\"POINT({lon} {lat})\"\n",
    "        except (ValueError, TypeError):\n",
    "            logging.warning(f\"Invalid geolocation format '{data['geolocation']}', setting to NULL.\")\n",
    "\n",
    "    visit_start_time, visit_end_time = None, None\n",
    "    if data.get('events'):\n",
    "        valid_events = []\n",
    "        for e in data['events']:\n",
    "            if 'event' in e and 'timestamp' in e['event']:\n",
    "                try:\n",
    "                    datetime.fromisoformat(e['event']['timestamp'].replace('Z', '+00:00'))\n",
    "                    valid_events.append(e)\n",
    "                except (ValueError, TypeError):\n",
    "                    logging.warning(f\"Invalid timestamp format, skipping event.\")\n",
    "        if valid_events:\n",
    "            sorted_events = sorted(\n",
    "                valid_events,\n",
    "                key=lambda x: datetime.fromisoformat(x['event']['timestamp'].replace('Z', '+00:00'))\n",
    "            )\n",
    "            visit_start_time = sorted_events[0]['event']['timestamp']\n",
    "            visit_end_time = sorted_events[-1]['event']['timestamp']\n",
    "\n",
    "    transformed_events = []\n",
    "    for event_data in data.get('events', []):\n",
    "        event_details = event_data.get('event', {})\n",
    "        details_payload = event_details.get('details', {})\n",
    "        event_type = event_details.get('event_type')\n",
    "        page_view_struct, add_cart_struct, purchase_struct = None, None, None\n",
    "        if event_type == 'page_view':\n",
    "            page_view_struct = details_payload\n",
    "        elif event_type == 'add_item_to_cart':\n",
    "            add_cart_struct = details_payload\n",
    "        elif event_type == 'purchase':\n",
    "            purchase_struct = details_payload\n",
    "        transformed_events.append({\n",
    "            'event_type': event_type,\n",
    "            'event_timestamp': event_details.get('timestamp'),\n",
    "            'page_view': page_view_struct,\n",
    "            'add_cart': add_cart_struct,\n",
    "            'purchase': purchase_struct\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'session_id': data.get('session_id'), 'user_id': data.get('user_id'),\n",
    "        'device_type': data.get('device_type'), 'geolocation': geolocation_wkt,\n",
    "        'user_agent': data.get('user_agent'), 'visit_start_time': visit_start_time,\n",
    "        'visit_end_time': visit_end_time, 'events': transformed_events\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac7ee4d-2a57-43dc-966c-cab995186dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PIPELINE DEFINITION ---\n",
    "def define_and_run_pipeline(pipeline_options, subscription_path, table_spec, gcs_output_path):\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        # Initial steps to read and decode messages are common to both branches\n",
    "        messages = (\n",
    "            pipeline\n",
    "            | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=subscription_path).with_output_types(bytes)\n",
    "            | 'Decode Messages' >> beam.Map(lambda msg_bytes: msg_bytes.decode('utf-8'))\n",
    "        )\n",
    "\n",
    "        # --- Branch 1: Transform and write to BigQuery ---\n",
    "        (\n",
    "            messages\n",
    "            | 'Transform to BigQuery Row' >> beam.Map(transform_json_to_bigquery_row)\n",
    "            | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "                table=table_spec, schema=TABLE_SCHEMA,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                method=beam.io.WriteToBigQuery.Method.STREAMING_INSERTS\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- Branch 2: Window and write raw JSON to Cloud Storage ---\n",
    "        (\n",
    "            messages\n",
    "            | 'Window into 1 Minute Batches' >> beam.WindowInto(window.FixedWindows(60))\n",
    "            | 'Write JSONL files to GCS' >> fileio.WriteToFiles(\n",
    "                path=gcs_output_path,\n",
    "                sink=JsonlSink(),\n",
    "                file_naming=fileio.default_file_naming(prefix=\"output\", suffix=\".jsonl\")\n",
    "            )\n",
    "        )\n",
    "    logging.info(\"Pipeline submitted to the runner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8eff481-b805-49c1-aac0-fc33b73dd507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:--- Starting Dataflow Job: streaming-multi-sink-1750364234 ---\n",
      "INFO:root:Streaming data to BigQuery table: jellyfish-training-demo-6:dsl_project.web_visits\n",
      "INFO:root:Archiving raw data to GCS path: gs://jellyfish-training-demo-6/dsl-project/jsonl_archive/*\n",
      "INFO:root:Runner defaulting to pickling library: cloudpickle.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "INFO:apache_beam.io.fileio:Added temporary directory gs://jellyfish-training-demo-6/dsl-project/temp/.temp84012352-a8d8-46de-9531-b0a987ba4dba\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/jupyter/.kernels/apache-beam-2.63.0/bin/python', '-m', 'build', '--no-isolation', '--sdist', '--outdir', '/tmp/tmplph1mdgq', '.']\n",
      "/jupyter/.kernels/apache-beam-2.63.0/bin/python: No module named build\n",
      "INFO:apache_beam.runners.portability.stager:Executing command: ['/jupyter/.kernels/apache-beam-2.63.0/bin/python', 'setup.py', 'sdist', '--dist-dir', '/tmp/tmplph1mdgq']\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Pipeline has additional dependencies to be installed in SDK worker container, consider using the SDK container image pre-building workflow to avoid repetitive installations. Learn more on https://cloud.google.com/dataflow/docs/guides/using-custom-containers#prebuild\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/dsl-project/staging/streaming-multi-sink-1750364234.1750364237.571292/workflow.tar.gz...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/dsl-project/staging/streaming-multi-sink-1750364234.1750364237.571292/workflow.tar.gz in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/dsl-project/staging/streaming-multi-sink-1750364234.1750364237.571292/submission_environment_dependencies.txt...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/dsl-project/staging/streaming-multi-sink-1750364234.1750364237.571292/submission_environment_dependencies.txt in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/dsl-project/staging/streaming-multi-sink-1750364234.1750364237.571292/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/dsl-project/staging/streaming-multi-sink-1750364234.1750364237.571292/pipeline.pb in 0 seconds.\n",
      "WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: -f,/home/jupyter/.local/share/jupyter/runtime/kernel-de795df5-d0f3-4a0f-82bf-a7be1dddc991.json. Ignore if flags are used for internal purposes.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "WARNING:apache_beam.options.pipeline_options:Unknown pipeline options received: -f,/home/jupyter/.local/share/jupyter/runtime/kernel-de795df5-d0f3-4a0f-82bf-a7be1dddc991.json. Ignore if flags are used for internal purposes.\n",
      "WARNING:apache_beam.options.pipeline_options:Discarding flag -f, single dash flags are not allowed.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20250619201717572154-7988'\n",
      " createTime: '2025-06-19T20:17:18.866894Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2025-06-19_13_17_18-9981886932791530248'\n",
      " location: 'us-central1'\n",
      " name: 'streaming-multi-sink-1750364234'\n",
      " projectId: 'jellyfish-training-demo-6'\n",
      " stageStates: []\n",
      " startTime: '2025-06-19T20:17:18.866894Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2025-06-19_13_17_18-9981886932791530248]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2025-06-19_13_17_18-9981886932791530248\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2025-06-19_13_17_18-9981886932791530248?project=jellyfish-training-demo-6\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-19_13_17_18-9981886932791530248 is in state JOB_STATE_PENDING\n",
      "WARNING:apache_beam.runners.dataflow.dataflow_runner:2025-06-19T20:17:20.962Z: JOB_MESSAGE_WARNING: Autoscaling is enabled for Dataflow Streaming Engine. Workers will scale between 1 and 100 unless maxNumWorkers is specified.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-19T20:17:26.301Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in us-central1-f.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-19T20:17:29.532Z: JOB_MESSAGE_BASIC: Running job using Streaming Engine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-19T20:17:29.818Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-19_13_17_18-9981886932791530248 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-19T20:20:42.996Z: JOB_MESSAGE_BASIC: All workers have finished the startup processes and began to receive work requests.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming data to BigQuery table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE_SPEC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchiving raw data to GCS path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGCS_OUTPUT_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mdefine_and_run_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubscription_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprojects/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROJECT_ID\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/subscriptions/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSUBSCRIPTION_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTABLE_SPEC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_output_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGCS_OUTPUT_PREFIX\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m, in \u001b[0;36mdefine_and_run_pipeline\u001b[0;34m(pipeline_options, subscription_path, table_spec, gcs_output_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefine_and_run_pipeline\u001b[39m(pipeline_options, subscription_path, table_spec, gcs_output_path):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mPipeline(options\u001b[38;5;241m=\u001b[39mpipeline_options) \u001b[38;5;28;01mas\u001b[39;00m pipeline:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Initial steps to read and decode messages are common to both branches\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         messages \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m             pipeline\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRead from Pub/Sub\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromPubSub(subscription\u001b[38;5;241m=\u001b[39msubscription_path)\u001b[38;5;241m.\u001b[39mwith_output_types(\u001b[38;5;28mbytes\u001b[39m)\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecode Messages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(\u001b[38;5;28;01mlambda\u001b[39;00m msg_bytes: msg_bytes\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m         )\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# --- Branch 1: Transform and write to BigQuery ---\u001b[39;00m\n",
      "File \u001b[0;32m/jupyter/.kernels/apache-beam-2.63.0/lib/python3.10/site-packages/apache_beam/pipeline.py:664\u001b[0m, in \u001b[0;36mPipeline.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mview_as(StandardOptions)\u001b[38;5;241m.\u001b[39mno_wait_until_finish:\n\u001b[0;32m--> 664\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    667\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob execution continues without waiting for completion.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    668\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Use \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_until_finish\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in PipelineResult to block\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    669\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m until finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/jupyter/.kernels/apache-beam-2.63.0/lib/python3.10/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:801\u001b[0m, in \u001b[0;36mDataflowPipelineResult.wait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m    799\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m--> 801\u001b[0m   \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# TODO: Merge the termination code in poll_for_job_completion and\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# is_in_terminal_state.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_terminal_state()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION BLOCK ---\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# --- 1. Set up project and resource details ---\n",
    "PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "SUBSCRIPTION_NAME = \"dsl-clickstream-push-beam\"\n",
    "DATASET_ID = \"dsl_project\"\n",
    "TABLE_ID = \"web_visits\"\n",
    "REGION = 'us-central1'\n",
    "GCS_BUCKET = \"jellyfish-training-demo-6\"\n",
    "GCS_PATH = \"dsl-project\"\n",
    "\n",
    "# --- 2. Configure pipeline options ---\n",
    "JOB_NAME = f'streaming-multi-sink-{int(time.time())}'\n",
    "TABLE_SPEC = f'{PROJECT_ID}:{DATASET_ID}.{TABLE_ID}'\n",
    "GCS_OUTPUT_PREFIX = f\"gs://{GCS_BUCKET}/{GCS_PATH}/jsonl_archive/\" # Path for the JSONL files\n",
    "\n",
    "options = PipelineOptions()\n",
    "standard_options = options.view_as(StandardOptions)\n",
    "standard_options.runner = 'DataflowRunner'\n",
    "standard_options.streaming = True\n",
    "\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = PROJECT_ID\n",
    "google_cloud_options.job_name = JOB_NAME\n",
    "google_cloud_options.staging_location = f\"gs://{GCS_BUCKET}/{GCS_PATH}/staging\"\n",
    "google_cloud_options.temp_location = f\"gs://{GCS_BUCKET}/{GCS_PATH}/temp\"\n",
    "google_cloud_options.region = REGION\n",
    "options.view_as(SetupOptions).setup_file = './setup.py'\n",
    "\n",
    "# --- 3. Run the pipeline ---\n",
    "logging.info(f\"--- Starting Dataflow Job: {JOB_NAME} ---\")\n",
    "logging.info(f\"Streaming data to BigQuery table: {TABLE_SPEC}\")\n",
    "logging.info(f\"Archiving raw data to GCS path: {GCS_OUTPUT_PREFIX}*\")\n",
    "\n",
    "define_and_run_pipeline(\n",
    "    pipeline_options=options,\n",
    "    subscription_path=f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\",\n",
    "    table_spec=TABLE_SPEC,\n",
    "    gcs_output_path=GCS_OUTPUT_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e88716-31fd-47c8-bea2-3bce647185b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
