{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9552900-bbd0-44aa-b7a6-aa93ce44d1be",
   "metadata": {},
   "source": [
    "Write an Apache Beam pipeline with the following requirements:\n",
    "\n",
    "Write the raw data to files in Google Cloud Storage at regular intervals.\n",
    "Parse the messages and write the data to BigQuery.\n",
    "Calculate page views by minute. Create a dashboard that reports this information.\n",
    "Run the pipeline in Dataflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123cb65-18b0-49cf-912b-dac35b8abdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: YOU NEED TO RUN THIS STEP ONLY ONCE \n",
    "\n",
    "#!pip install --quiet \"apache-beam[gcp]\" \"google-cloud-pubsub\" \"notebook\"\n",
    "!pip install --upgrade apache-beam[gcp]\n",
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfc9623-40b6-40c1-a629-d50b99462788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pubsub subscription to be used for the beam pipeline\n",
    "# IMPORTANT: YOU NEED TO RUN THIS STEP ONLY ONCE \n",
    "\n",
    "gcloud pubsub subscriptions create dsl-clickstream-push-beam \\\n",
    "    --topic=dsl-project-clickstream\n",
    "\n",
    "# this is a pull subscription, even though we are building a streaming pipeline\n",
    "# dataflow will handle the processing of data as quickly as possible, using the proper IO pubsub components within Beam model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae63066-08ca-46f9-85ba-6f5b5f91c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, DirectOptions\n",
    "import logging\n",
    "import time\n",
    "from apache_beam.io import fileio\n",
    "from apache_beam.transforms import window\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from datetime import datetime\n",
    "from apache_beam.io import fileio\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d89f95-d812-4a97-9761-9ff63303bfbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Python Script to Launch Dataflow Job ---\n",
    "\n",
    "# --- BIGQUERY TABLE SCHEMA ---\n",
    "TABLE_SCHEMA = {\n",
    "    'fields': [\n",
    "        {'name': 'session_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'user_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'device_type', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'geolocation', 'type': 'GEOGRAPHY', 'mode': 'NULLABLE'},\n",
    "        {'name': 'user_agent', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'visit_start_time', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "        {'name': 'visit_end_time', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "        {\n",
    "            'name': 'events', 'type': 'RECORD', 'mode': 'REPEATED',\n",
    "            'fields': [\n",
    "                {'name': 'event_type', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                {'name': 'event_timestamp', 'type': 'TIMESTAMP', 'mode': 'NULLABLE'},\n",
    "                {\n",
    "                    'name': 'page_view', 'type': 'RECORD', 'mode': 'NULLABLE',\n",
    "                    'fields': [\n",
    "                        {'name': 'page_url', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'referrer_url', 'type': 'STRING', 'mode': 'NULLABLE'}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    'name': 'add_cart', 'type': 'RECORD', 'mode': 'NULLABLE',\n",
    "                    'fields': [\n",
    "                        {'name': 'product_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'product_name', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'category', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'price', 'type': 'FLOAT64', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'quantity', 'type': 'INT64', 'mode': 'NULLABLE'}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    'name': 'purchase', 'type': 'RECORD', 'mode': 'NULLABLE',\n",
    "                    'fields': [\n",
    "                        {'name': 'order_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'amount', 'type': 'FLOAT64', 'mode': 'NULLABLE'},\n",
    "                        {'name': 'currency', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                        {\n",
    "                            'name': 'items', 'type': 'RECORD', 'mode': 'REPEATED',\n",
    "                            'fields': [\n",
    "                                {'name': 'product_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'product_name', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'category', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'price', 'type': 'FLOAT64', 'mode': 'NULLABLE'},\n",
    "                                {'name': 'quantity', 'type': 'INT64', 'mode': 'NULLABLE'}\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75877175-e2eb-4638-b639-6a89a10fdbeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CUSTOM SINK FOR WRITING JSONL FILES TO GCS ---\n",
    "    \n",
    "class JsonlSink(fileio.FileSink):\n",
    "    \"\"\"\n",
    "    JsonlSink defines the behavior for writing PCollection elements to a file in\n",
    "    JSON Lines (JSONL) format. It inherits from fileio.FileSink and implements\n",
    "    the open, write, and flush methods to handle file I/O.\n",
    "    \"\"\"\n",
    "    \"\"\"A sink that writes JSONL records to a file.\"\"\"\n",
    "    def __init__(self):\n",
    "        # The sink needs a coder to encode the strings into bytes.\n",
    "        self._coder = beam.coders.StrUtf8Coder()\n",
    "        self._fh = None\n",
    "\n",
    "    def open(self, fh):\n",
    "        \"\"\"This method is called when a new file is created for writing.\"\"\"\n",
    "        self._fh = fh\n",
    "\n",
    "    def write(self, record):\n",
    "        \"\"\"\n",
    "        This method is called for each element. It uses the file handle\n",
    "        that was saved by the open() method.\n",
    "        \"\"\"\n",
    "        if self._fh is None:\n",
    "            raise RuntimeError('Sink open() was not called before write()')\n",
    "        self._fh.write(self._coder.encode(record) + b'\\n')\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"\n",
    "        This method is called before a file is closed. It uses the file handle\n",
    "        that was saved by the open() method.\n",
    "        \"\"\"\n",
    "        if self._fh is not None:\n",
    "            self._fh.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac489445-c462-4cf6-b2a6-3774a4c867b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- CUSTOM TRANSFORMATION LOGIC FOR BIGQUERY ---\n",
    "def transform_json_to_bigquery_row(message_body: str) -> dict:\n",
    "    data = json.loads(message_body)\n",
    "    geolocation_wkt = None\n",
    "    if 'geolocation' in data and data['geolocation']:\n",
    "        try:\n",
    "            lat, lon = map(float, data['geolocation'].split(','))\n",
    "            geolocation_wkt = f\"POINT({lon} {lat})\"\n",
    "        except (ValueError, TypeError):\n",
    "            logging.warning(f\"Invalid geolocation format '{data['geolocation']}', setting to NULL.\")\n",
    "\n",
    "    visit_start_time, visit_end_time = None, None\n",
    "    if data.get('events'):\n",
    "        valid_events = []\n",
    "        for e in data['events']:\n",
    "            if 'event' in e and 'timestamp' in e['event']:\n",
    "                try:\n",
    "                    datetime.fromisoformat(e['event']['timestamp'].replace('Z', '+00:00'))\n",
    "                    valid_events.append(e)\n",
    "                except (ValueError, TypeError):\n",
    "                    logging.warning(f\"Invalid timestamp format, skipping event.\")\n",
    "        if valid_events:\n",
    "            sorted_events = sorted(\n",
    "                valid_events,\n",
    "                key=lambda x: datetime.fromisoformat(x['event']['timestamp'].replace('Z', '+00:00'))\n",
    "            )\n",
    "            visit_start_time = sorted_events[0]['event']['timestamp']\n",
    "            visit_end_time = sorted_events[-1]['event']['timestamp']\n",
    "\n",
    "    transformed_events = []\n",
    "    for event_data in data.get('events', []):\n",
    "        event_details = event_data.get('event', {})\n",
    "        details_payload = event_details.get('details', {})\n",
    "        event_type = event_details.get('event_type')\n",
    "        page_view_struct, add_cart_struct, purchase_struct = None, None, None\n",
    "        if event_type == 'page_view':\n",
    "            page_view_struct = details_payload\n",
    "        elif event_type == 'add_item_to_cart':\n",
    "            add_cart_struct = details_payload\n",
    "        elif event_type == 'purchase':\n",
    "            purchase_struct = details_payload\n",
    "        transformed_events.append({\n",
    "            'event_type': event_type,\n",
    "            'event_timestamp': event_details.get('timestamp'),\n",
    "            'page_view': page_view_struct,\n",
    "            'add_cart': add_cart_struct,\n",
    "            'purchase': purchase_struct\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'session_id': data.get('session_id'), 'user_id': data.get('user_id'),\n",
    "        'device_type': data.get('device_type'), 'geolocation': geolocation_wkt,\n",
    "        'user_agent': data.get('user_agent'), 'visit_start_time': visit_start_time,\n",
    "        'visit_end_time': visit_end_time, 'events': transformed_events\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7ee4d-2a57-43dc-966c-cab995186dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- PIPELINE DEFINITION ---\n",
    "def define_and_run_pipeline(pipeline_options, subscription_path, table_spec, gcs_output_path):\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        # Initial steps to read and decode messages are common to both branches\n",
    "        messages = (\n",
    "            pipeline\n",
    "            | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=subscription_path).with_output_types(bytes)\n",
    "            | 'Decode Messages' >> beam.Map(lambda msg_bytes: msg_bytes.decode('utf-8'))\n",
    "        )\n",
    "\n",
    "        # --- Branch 1: Transform and write to BigQuery ---\n",
    "        (\n",
    "            messages\n",
    "            | 'Transform to BigQuery Row' >> beam.Map(transform_json_to_bigquery_row)\n",
    "            | 'Write to BigQuery' >> beam.io.WriteToBigQuery(\n",
    "                table=table_spec, schema=TABLE_SCHEMA,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                method=beam.io.WriteToBigQuery.Method.STREAMING_INSERTS\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- Branch 2: Window and write raw JSON to Cloud Storage ---\n",
    "        (\n",
    "            messages\n",
    "            | 'Window into 1 Minute Batches' >> beam.WindowInto(window.FixedWindows(60))\n",
    "            | 'Write JSONL files to GCS' >> fileio.WriteToFiles(\n",
    "                path=gcs_output_path,\n",
    "                sink=JsonlSink(),\n",
    "                file_naming=fileio.default_file_naming(prefix=\"visit\", suffix=\".jsonl\")\n",
    "            )\n",
    "        )\n",
    "    logging.info(\"Pipeline submitted to the runner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8eff481-b805-49c1-aac0-fc33b73dd507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-24_07_26_01-8008251263561795308 is in state JOB_STATE_CANCELLING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T15:07:20.575Z: JOB_MESSAGE_BASIC: Cancel request is committed for workflow job: 2025-06-24_07_26_01-8008251263561795308.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T15:07:20.817Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T15:07:20.872Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStreaming data to BigQuery table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE_SPEC\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchiving raw data to GCS path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGCS_OUTPUT_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mdefine_and_run_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubscription_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprojects/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROJECT_ID\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/subscriptions/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSUBSCRIPTION_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTABLE_SPEC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_output_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGCS_OUTPUT_PREFIX\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mdefine_and_run_pipeline\u001b[0;34m(pipeline_options, subscription_path, table_spec, gcs_output_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefine_and_run_pipeline\u001b[39m(pipeline_options, subscription_path, table_spec, gcs_output_path):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mPipeline(options\u001b[38;5;241m=\u001b[39mpipeline_options) \u001b[38;5;28;01mas\u001b[39;00m pipeline:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Initial steps to read and decode messages are common to both branches\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         messages \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m             pipeline\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRead from Pub/Sub\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mReadFromPubSub(subscription\u001b[38;5;241m=\u001b[39msubscription_path)\u001b[38;5;241m.\u001b[39mwith_output_types(\u001b[38;5;28mbytes\u001b[39m)\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDecode Messages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mMap(\u001b[38;5;28;01mlambda\u001b[39;00m msg_bytes: msg_bytes\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m         )\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# --- Branch 1: Transform and write to BigQuery ---\u001b[39;00m\n",
      "File \u001b[0;32m/jupyter/.kernels/apache-beam-2.63.0/lib/python3.10/site-packages/apache_beam/pipeline.py:664\u001b[0m, in \u001b[0;36mPipeline.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mview_as(StandardOptions)\u001b[38;5;241m.\u001b[39mno_wait_until_finish:\n\u001b[0;32m--> 664\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    667\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob execution continues without waiting for completion.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    668\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Use \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_until_finish\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in PipelineResult to block\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    669\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m until finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/jupyter/.kernels/apache-beam-2.63.0/lib/python3.10/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:801\u001b[0m, in \u001b[0;36mDataflowPipelineResult.wait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m    799\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m--> 801\u001b[0m   \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# TODO: Merge the termination code in poll_for_job_completion and\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# is_in_terminal_state.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_terminal_state()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- MAIN EXECUTION BLOCK ---\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# --- 1. Set up project and resource details ---\n",
    "PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "SUBSCRIPTION_NAME = \"dsl-clickstream-push-beam\"\n",
    "DATASET_ID = \"dsl_project\"\n",
    "TABLE_ID = \"web_visits\"\n",
    "REGION = 'us-central1'\n",
    "GCS_BUCKET = \"jellyfish-training-demo-6\"\n",
    "GCS_PATH = \"dsl-project\"\n",
    "\n",
    "# --- 2. Configure pipeline options ---\n",
    "JOB_NAME = f'streaming-multi-sink-{int(time.time())}'\n",
    "TABLE_SPEC = f'{PROJECT_ID}:{DATASET_ID}.{TABLE_ID}'\n",
    "GCS_OUTPUT_PREFIX = f\"gs://{GCS_BUCKET}/{GCS_PATH}/jsonl_archive/\" # Path for the JSONL files\n",
    "\n",
    "options = PipelineOptions()\n",
    "standard_options = options.view_as(StandardOptions)\n",
    "standard_options.runner = 'DataflowRunner'\n",
    "standard_options.streaming = True\n",
    "\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = PROJECT_ID\n",
    "google_cloud_options.job_name = JOB_NAME\n",
    "google_cloud_options.staging_location = f\"gs://{GCS_BUCKET}/{GCS_PATH}/staging\"\n",
    "google_cloud_options.temp_location = f\"gs://{GCS_BUCKET}/{GCS_PATH}/temp\"\n",
    "google_cloud_options.region = REGION\n",
    "options.view_as(SetupOptions).setup_file = './setup.py'\n",
    "\n",
    "# --- 3. Run the pipeline ---\n",
    "logging.info(f\"--- Starting Dataflow Job: {JOB_NAME} ---\")\n",
    "logging.info(f\"Streaming data to BigQuery table: {TABLE_SPEC}\")\n",
    "logging.info(f\"Archiving raw data to GCS path: {GCS_OUTPUT_PREFIX}*\")\n",
    "\n",
    "define_and_run_pipeline(\n",
    "    pipeline_options=options,\n",
    "    subscription_path=f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\",\n",
    "    table_spec=TABLE_SPEC,\n",
    "    gcs_output_path=GCS_OUTPUT_PREFIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8052f0-23bb-454c-8c1e-de284527aef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2: Securely View Pipeline Output\n",
    "# Run this cell a few minutes after the pipeline has started.\n",
    "\n",
    "\n",
    "\n",
    "# --- You may need to install the library first ---\n",
    "# !pip install google-cloud-storage\n",
    "\n",
    "# --- Configuration (should match your pipeline) ---\n",
    "GCS_BUCKET = \"jellyfish-training-demo-6\" \n",
    "GCS_PATH = \"dsl-project/dataflow_run\" # The output path for your data\n",
    "\n",
    "# --- Use the Client Library to access GCS ---\n",
    "# This uses your gcloud credentials automatically.\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(GCS_BUCKET)\n",
    "\n",
    "# List all files in the output directory\n",
    "blobs = list(bucket.list_blobs(prefix=GCS_PATH))\n",
    "\n",
    "if not blobs:\n",
    "    print(\"No output files found yet. Please wait a few more minutes.\")\n",
    "    print(f\"Searched in: gs://{GCS_BUCKET}/{GCS_PATH}\")\n",
    "else:\n",
    "    # Find the most recently created file\n",
    "    latest_blob = sorted(blobs, key=lambda b: b.time_created, reverse=True)[0]\n",
    "    \n",
    "    print(f\"--- Displaying contents of the latest output file ---\")\n",
    "    print(f\"File: gs://{GCS_BUCKET}/{latest_blob.name}\")\n",
    "    print(f\"Created at: {latest_blob.time_created}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Download and print the content\n",
    "    content = latest_blob.download_as_text()\n",
    "    \n",
    "    # Print each JSON line\n",
    "    for line in content.strip().split('\\n'):\n",
    "        # Pretty-print the JSON for readability\n",
    "        parsed_json = json.loads(line)\n",
    "        print(json.dumps(parsed_json, indent=2))\n",
    "        \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e88716-31fd-47c8-bea2-3bce647185b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
