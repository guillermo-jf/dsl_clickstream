{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777c50f7-09d3-4b0d-86ef-35b3e1679126",
   "metadata": {},
   "source": [
    "How This New Pipeline Works\n",
    "\n",
    "Read and Decode: It starts the same way, by reading and decoding the raw JSON messages from your Pub/Sub subscription.\n",
    "ExtractPageViews DoFn:\n",
    "This is a crucial new step. Instead of processing the whole session, this DoFn iterates through the events array within each message.\n",
    "\n",
    "If an event is a 'page_view', it uses beam.utils.timestamp.Timestamp.from_rfc3339 to parse the event's timestamp.\n",
    "\n",
    "It then yields a TimestampedValue. This is a special Beam object that contains a value (in this case, just the number 1) and an associated timestamp. This tells Beam to use the event's time for windowing, not the time the message was processed.\n",
    "\n",
    "WindowInto(window.FixedWindows(60)): This groups all the 1s that were yielded into fixed, non-overlapping 60-second windows based on their assigned event timestamps.\n",
    "\n",
    "Count.Globally(): For each one-minute window, this transform simply counts how many elements (1s) it contains. This gives you the total page views for that minute.\n",
    "\n",
    "\n",
    "LogAggregation DoFn:\n",
    "\n",
    "This final DoFn receives the count for each completed window.\n",
    "\n",
    "It accesses the window's start time via beam.DoFn.WindowParam.\n",
    "\n",
    "It formats a user-friendly string and uses logging.info() to write it out. When running on Dataflow, these logs are automatically sent to Google Cloud Logging and can be viewed on the job's log page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a91fdf81-ea7a-4eda-b8de-95a2ea29e1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750719945.402799   36612 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750719950.484992   36612 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --quiet \"apache-beam[gcp]\"\n",
    "!pip install -q google-cloud-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb435a-cfc7-48d7-9603-051c97dd7ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use a terminal window in your project to create this firewall rule\n",
    "\n",
    "gcloud compute firewall-rules create dataflow-allow-egress-to-google-apis \\\n",
    "    --network=default \\\n",
    "    --action=ALLOW \\\n",
    "    --direction=EGRESS \\\n",
    "    --destination-ranges=0.0.0.0/0 \\\n",
    "    --rules=tcp:443 \\\n",
    "    --priority=1000 \\\n",
    "    --project=jellyfish-training-demo-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f996fb4-6663-4df9-948f-46005d1b3ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Runner defaulting to pickling library: cloudpickle.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Submitting Dataflow Streaming Pipeline: pageview-counter-notebook-job-1750732750 ---\n",
      "--- Reading from: projects/jellyfish-training-demo-6/subscriptions/dsl-clickstream-ddos ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class '__main__.LogAggregation'>)\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.transforms.core:('No iterator is returned by the process method in %s.', <class '__main__.LogAggregation'>)\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Pipeline has additional dependencies to be installed in SDK worker container, consider using the SDK container image pre-building workflow to avoid repetitive installations. Learn more on https://cloud.google.com/dataflow/docs/guides/using-custom-containers#prebuild\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/staging/pageview-counter-notebook-job-1750732750.1750732752.323969/submission_environment_dependencies.txt...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/staging/pageview-counter-notebook-job-1750732750.1750732752.323969/submission_environment_dependencies.txt in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/staging/pageview-counter-notebook-job-1750732750.1750732752.323969/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/staging/pageview-counter-notebook-job-1750732750.1750732752.323969/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20250624023912325377-9150'\n",
      " createTime: '2025-06-24T02:39:13.350254Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2025-06-23_19_39_12-18214927796337867699'\n",
      " location: 'us-central1'\n",
      " name: 'pageview-counter-notebook-job-1750732750'\n",
      " projectId: 'jellyfish-training-demo-6'\n",
      " stageStates: []\n",
      " startTime: '2025-06-24T02:39:13.350254Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_STREAMING, 2)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2025-06-23_19_39_12-18214927796337867699]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2025-06-23_19_39_12-18214927796337867699\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2025-06-23_19_39_12-18214927796337867699?project=jellyfish-training-demo-6\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-23_19_39_12-18214927796337867699 is in state JOB_STATE_PENDING\n",
      "WARNING:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T02:39:14.824Z: JOB_MESSAGE_WARNING: Autoscaling is enabled for Dataflow Streaming Engine. Workers will scale between 1 and 100 unless maxNumWorkers is specified.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T02:40:25.776Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-2 in us-central1-a.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T02:40:28.295Z: JOB_MESSAGE_BASIC: Running job using Streaming Engine\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T02:40:28.563Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-23_19_39_12-18214927796337867699 is in state JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:41:00Z, there were 58 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:41:00Z\", \"page_view_count\": 58, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:41:00Z, there were 22 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:41:00Z, there were 15 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:42:00Z, there were 16 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:42:00Z, there were 52 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:42:00Z\", \"page_view_count\": 52, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:42:00Z, there were 17 page views.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T02:43:48.049Z: JOB_MESSAGE_BASIC: All workers have finished the startup processes and began to receive work requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:43:00Z, there were 23 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:43:00Z, there were 20 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:43:00Z, there were 20 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:43:00Z\", \"page_view_count\": 20, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:44:00Z, there were 19 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:44:00Z, there were 23 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:44:00Z\", \"page_view_count\": 23, \"metric\": \"page_views_per_minute\"}}PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:44:00Z, there were 23 page views.\n",
      "\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:45:00Z, there were 19 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:45:00Z, there were 16 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:45:00Z\", \"page_view_count\": 16, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:45:00Z, there were 16 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:46:00Z, there were 23 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:46:00Z, there were 21 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:46:00Z\", \"page_view_count\": 21, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:46:00Z, there were 21 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:47:00Z, there were 25 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:47:00Z, there were 24 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:47:00Z\", \"page_view_count\": 24, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:47:00Z, there were 24 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:48:00Z, there were 20 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:48:00Z, there were 21 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:48:00Z\", \"page_view_count\": 21, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:48:00Z, there were 21 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:49:00Z, there were 21 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:49:00Z, there were 24 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:49:00Z\", \"page_view_count\": 24, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:49:00Z, there were 24 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:50:00Z, there were 20 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:50:00Z, there were 22 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T02:50:00Z\", \"page_view_count\": 22, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T02:50:00Z, there were 22 page views.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    138\u001b[0m     logging\u001b[38;5;241m.\u001b[39mgetLogger()\u001b[38;5;241m.\u001b[39msetLevel(logging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[0;32m--> 139\u001b[0m     \u001b[43mrun_dataflow_from_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 118\u001b[0m, in \u001b[0;36mrun_dataflow_from_notebook\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Reading from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubscription_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# The 'with' block is crucial for proper pipeline execution and teardown.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m beam\u001b[38;5;241m.\u001b[39mPipeline(options\u001b[38;5;241m=\u001b[39mpipeline_options) \u001b[38;5;28;01mas\u001b[39;00m pipeline:\n\u001b[1;32m    119\u001b[0m     (\n\u001b[1;32m    120\u001b[0m         pipeline\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;66;03m# The Dataflow runner fully supports ReadFromPubSub.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;241m|\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormat and Log Count\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m>>\u001b[39m beam\u001b[38;5;241m.\u001b[39mParDo(LogAggregation())\n\u001b[1;32m    127\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Job submitted successfully! You can monitor it in the Dataflow UI. ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/jupyter/.kernels/apache-beam-2.63.0/lib/python3.10/site-packages/apache_beam/pipeline.py:664\u001b[0m, in \u001b[0;36mPipeline.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mview_as(StandardOptions)\u001b[38;5;241m.\u001b[39mno_wait_until_finish:\n\u001b[0;32m--> 664\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    667\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob execution continues without waiting for completion.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    668\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Use \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait_until_finish\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in PipelineResult to block\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    669\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m until finished.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/jupyter/.kernels/apache-beam-2.63.0/lib/python3.10/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:801\u001b[0m, in \u001b[0;36mDataflowPipelineResult.wait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m    799\u001b[0m thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m--> 801\u001b[0m   \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# TODO: Merge the termination code in poll_for_job_completion and\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# is_in_terminal_state.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_terminal_state()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:16:55.491Z: JOB_MESSAGE_BASIC: Cancel request is committed for workflow job: 2025-06-23_19_39_12-18214927796337867699.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:16:55.649Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:16:55.702Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-23_19_39_12-18214927796337867699 is in state JOB_STATE_CANCELLING\n"
     ]
    }
   ],
   "source": [
    "# this cell runs to code in a dataflow job\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms import window\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# =================================================================\n",
    "# 1. DEFINE PIPELINE LOGIC (DoFns)\n",
    "# =================================================================\n",
    "\n",
    "class ExtractPageViews(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Parses incoming JSON from Pub/Sub and yields 1 for each 'page_view' event.\n",
    "    It uses the original timestamp from the data for event-time windowing.\n",
    "    \"\"\"\n",
    "    def process(self, element, *args, **kwargs):\n",
    "        try:\n",
    "            # The element from ReadFromPubSub is a byte string.\n",
    "            data_str = element.decode('utf-8')\n",
    "            data = json.loads(data_str)\n",
    "            \n",
    "            events = data.get('events', [])\n",
    "            for event_data in events:\n",
    "                event = event_data.get('event', {})\n",
    "                if event.get('event_type') == 'page_view':\n",
    "                    event_timestamp_str = event.get('timestamp')\n",
    "                    if event_timestamp_str:\n",
    "                        # --- FIX APPLIED HERE ---\n",
    "                        # The previous logic was incorrectly handling timezone-aware timestamps.\n",
    "                        # The datetime.fromisoformat() function correctly parses strings\n",
    "                        # with timezone offsets (like '-04:00'). This timezone-aware\n",
    "                        # datetime object can be passed directly to Beam's timestamp utility,\n",
    "                        # which will handle the conversion to UTC correctly.\n",
    "                        \n",
    "                        # 1. Parse the string directly into a timezone-aware datetime object.\n",
    "                        aware_dt_object = datetime.fromisoformat(event_timestamp_str)\n",
    "                        \n",
    "                        # 2. Create a Beam Timestamp. This will be correctly converted to UTC.\n",
    "                        event_timestamp = beam.utils.timestamp.Timestamp.from_datetime(aware_dt_object)\n",
    "                        \n",
    "                        yield beam.window.TimestampedValue(1, event_timestamp)\n",
    "                        \n",
    "        except Exception as e:\n",
    "            # Dataflow will automatically log errors to Cloud Logging.\n",
    "            logging.error(f\"Error parsing element: {e} - Element: {str(element)[:200]}\")\n",
    "\n",
    "\n",
    "class LogAggregation(beam.DoFn):\n",
    "    \"\"\"\n",
    "    Formats the aggregated count and logs it via structured logging.\n",
    "    \"\"\"\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_rfc3339()\n",
    "        page_view_count = element\n",
    "        \n",
    "        # This structured log format is automatically parsed by Google Cloud's logging agent\n",
    "        # on the Dataflow workers.\n",
    "        log_entry = {\n",
    "            \"message\": (\n",
    "                f\"PIPELINE OUTPUT: In the 1-minute window starting at {window_start}, \"\n",
    "                f\"there were {page_view_count} page views.\"\n",
    "            ),\n",
    "            \"severity\": \"INFO\",\n",
    "            \"jsonPayload\": {\n",
    "                \"window_start\": window_start,\n",
    "                \"page_view_count\": page_view_count,\n",
    "                \"metric\": \"page_views_per_minute\"\n",
    "            }\n",
    "        }\n",
    "        # Printing the JSON string is the correct way to log to Cloud Logging from Dataflow.\n",
    "        print(json.dumps(log_entry))\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 2. DEFINE PIPELINE OPTIONS AND EXECUTION FOR JUPYTER NOTEBOOK\n",
    "# =================================================================\n",
    "\n",
    "def run_dataflow_from_notebook():\n",
    "    \"\"\"Defines and runs the Dataflow pipeline from a notebook cell.\"\"\"\n",
    "\n",
    "    # --- Manually define all pipeline options for notebook execution ---\n",
    "    \n",
    "    # --- REQUIRED: Replace with your details if they are different ---\n",
    "    PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "    SUBSCRIPTION_NAME = \"dsl-clickstream-ddos\"\n",
    "    # IMPORTANT: Make sure this GCS bucket exists in your project.\n",
    "    BUCKET_NAME = \"jellyfish-training-demo-6\" \n",
    "    REGION = \"us-central1\"\n",
    "    \n",
    "    # Construct the full subscription path\n",
    "    subscription_path = f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\"\n",
    "    \n",
    "    # Create a unique job name using a timestamp\n",
    "    job_name = f\"pageview-counter-notebook-job-{int(time.time())}\"\n",
    "\n",
    "    # Build the list of arguments as if they were coming from the command line.\n",
    "    # This is the standard way to launch Dataflow programmatically.\n",
    "    pipeline_args = [\n",
    "        f'--runner=DataflowRunner',\n",
    "        f'--project={PROJECT_ID}',\n",
    "        f'--region={REGION}',\n",
    "        f'--job_name={job_name}',\n",
    "        f'--temp_location=gs://{BUCKET_NAME}/temp',\n",
    "        f'--staging_location=gs://{BUCKET_NAME}/staging',\n",
    "        '--streaming' # Enable streaming mode\n",
    "    ]\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "    print(f\"--- Submitting Dataflow Streaming Pipeline: {job_name} ---\")\n",
    "    print(f\"--- Reading from: {subscription_path} ---\")\n",
    "\n",
    "    # The 'with' block is crucial for proper pipeline execution and teardown.\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        (\n",
    "            pipeline\n",
    "            # The Dataflow runner fully supports ReadFromPubSub.\n",
    "            | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=subscription_path)\n",
    "            | 'Extract Page Views' >> beam.ParDo(ExtractPageViews())\n",
    "            | 'Window into One Minute Batches' >> beam.WindowInto(window.FixedWindows(60))\n",
    "            | 'Count Events per Minute' >> beam.combiners.Count.Globally().without_defaults()\n",
    "            | 'Format and Log Count' >> beam.ParDo(LogAggregation())\n",
    "        )\n",
    "    print(\"--- Job submitted successfully! You can monitor it in the Dataflow UI. ---\")\n",
    "    print(f\"--- Job Name: {job_name} ---\")\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# 3. SCRIPT ENTRY POINT\n",
    "# =================================================================\n",
    "# This block will now execute the function defined above when you run the cell.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_dataflow_from_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c94aac59-d65b-467c-b149-39bdcbd8b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Runner defaulting to pickling library: cloudpickle.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Submitting MINIMAL test job: minimal-test-job-1750734980 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in temp_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "WARNING:apache_beam.options.pipeline_options:Bucket specified in staging_location has soft-delete policy enabled. To avoid being billed for unnecessary storage costs, turn off the soft delete feature on buckets that your Dataflow jobs use for temporary and staging storage. For more information, see https://cloud.google.com/storage/docs/use-soft-delete#remove-soft-delete-policy.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Pipeline has additional dependencies to be installed in SDK worker container, consider using the SDK container image pre-building workflow to avoid repetitive installations. Learn more on https://cloud.google.com/dataflow/docs/guides/using-custom-containers#prebuild\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/staging/minimal-test-job-1750734980.1750734982.540984/submission_environment_dependencies.txt...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/staging/minimal-test-job-1750734980.1750734982.540984/submission_environment_dependencies.txt in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Starting GCS upload to gs://jellyfish-training-demo-6/staging/minimal-test-job-1750734980.1750734982.540984/pipeline.pb...\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Completed GCS upload to gs://jellyfish-training-demo-6/staging/minimal-test-job-1750734980.1750734982.540984/pipeline.pb in 0 seconds.\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Create job: <Job\n",
      " clientRequestId: '20250624031622542593-3800'\n",
      " createTime: '2025-06-24T03:16:23.428178Z'\n",
      " currentStateTime: '1970-01-01T00:00:00Z'\n",
      " id: '2025-06-23_20_16_23-206428047784032306'\n",
      " location: 'us-central1'\n",
      " name: 'minimal-test-job-1750734980'\n",
      " projectId: 'jellyfish-training-demo-6'\n",
      " stageStates: []\n",
      " startTime: '2025-06-24T03:16:23.428178Z'\n",
      " steps: []\n",
      " tempFiles: []\n",
      " type: TypeValueValuesEnum(JOB_TYPE_BATCH, 1)>\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Created job with id: [2025-06-23_20_16_23-206428047784032306]\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:Submitted job: 2025-06-23_20_16_23-206428047784032306\n",
      "INFO:apache_beam.runners.dataflow.internal.apiclient:To access the Dataflow monitoring console, please navigate to https://console.cloud.google.com/dataflow/jobs/us-central1/2025-06-23_20_16_23-206428047784032306?project=jellyfish-training-demo-6\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-23_20_16_23-206428047784032306 is in state JOB_STATE_PENDING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:16:27.942Z: JOB_MESSAGE_BASIC: Worker configuration: n1-standard-1 in us-central1-c.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:16:29.126Z: JOB_MESSAGE_BASIC: Executing operation Create Test Data/Impulse+Create Test Data/FlatMap(<lambda at core.py:3970>)+Create Test Data/Map(decode)+Log Output\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:16:29.196Z: JOB_MESSAGE_BASIC: Starting 1 workers in us-central1...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-23_20_16_23-206428047784032306 is in state JOB_STATE_RUNNING\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:19:56.381Z: JOB_MESSAGE_BASIC: All workers have finished the startup processes and began to receive work requests.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:19:57.313Z: JOB_MESSAGE_BASIC: Finished operation Create Test Data/Impulse+Create Test Data/FlatMap(<lambda at core.py:3970>)+Create Test Data/Map(decode)+Log Output\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:19:57.507Z: JOB_MESSAGE_BASIC: Stopping worker pool...\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:2025-06-24T03:22:34.631Z: JOB_MESSAGE_BASIC: Worker pool stopped.\n",
      "INFO:apache_beam.runners.dataflow.dataflow_runner:Job 2025-06-23_20_16_23-206428047784032306 is in state JOB_STATE_DONE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Minimal job submitted successfully! ---\n",
      "--- Job Name: minimal-test-job-1750734980 ---\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:36:00Z, there were 14 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:36:00Z, there were 27 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:36:00Z\", \"page_view_count\": 27, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:36:00Z, there were 6 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:37:00Z, there were 21 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:37:00Z, there were 40 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:37:00Z\", \"page_view_count\": 40, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:37:00Z, there were 22 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:38:00Z, there were 24 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:38:00Z, there were 24 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:38:00Z\", \"page_view_count\": 24, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:38:00Z, there were 22 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:39:00Z, there were 14 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:39:00Z\", \"page_view_count\": 14, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:39:00Z, there were 14 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:39:00Z, there were 9 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:47:00Z, there were 8 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:47:00Z\", \"page_view_count\": 8, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:47:00Z, there were 2 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:48:00Z, there were 21 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:48:00Z, there were 60 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:48:00Z\", \"page_view_count\": 60, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:48:00Z, there were 18 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:49:00Z, there were 26 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:49:00Z, there were 31 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:49:00Z\", \"page_view_count\": 31, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:49:00Z, there were 17 page views.\n",
      "{\"message\": \"PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:50:00Z, there were 23 page views.\", \"severity\": \"INFO\", \"jsonPayload\": {\"window_start\": \"2025-06-24T03:50:00Z\", \"page_view_count\": 23, \"metric\": \"page_views_per_minute\"}}\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:50:00Z, there were 19 page views.\n",
      "PIPELINE OUTPUT: In the 1-minute window starting at 2025-06-24T03:50:00Z, there were 19 page views.\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def run_minimal_dataflow_job():\n",
    "    # --- Manually define options for a minimal test job ---\n",
    "    PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "    BUCKET_NAME = \"jellyfish-training-demo-6\"\n",
    "    REGION = \"us-central1\"\n",
    "    job_name = f\"minimal-test-job-{int(time.time())}\"\n",
    "\n",
    "    pipeline_args = [\n",
    "        f'--runner=DataflowRunner',\n",
    "        f'--project={PROJECT_ID}',\n",
    "        f'--region={REGION}',\n",
    "        f'--job_name={job_name}',\n",
    "        f'--temp_location=gs://{BUCKET_NAME}/temp',\n",
    "        f'--staging_location=gs://{BUCKET_NAME}/staging',\n",
    "        # NOTE: This is a batch job, so no '--streaming' flag is needed.\n",
    "    ]\n",
    "\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "\n",
    "    print(f\"--- Submitting MINIMAL test job: {job_name} ---\")\n",
    "\n",
    "    try:\n",
    "        with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "            (\n",
    "                pipeline\n",
    "                | 'Create Test Data' >> beam.Create(['hello world'])\n",
    "                | 'Log Output' >> beam.Map(print)\n",
    "            )\n",
    "        \n",
    "        print(\"--- Minimal job submitted successfully! ---\")\n",
    "        print(f\"--- Job Name: {job_name} ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"--- An error occurred during submission ---\")\n",
    "        print(e)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_minimal_dataflow_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bcb90-2f6d-4208-bbee-e5e02fcac838",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 ddos_pipeline.py \\\n",
    "    --runner DataflowRunner \\\n",
    "    --project jellyfish-training-demo-6 \\\n",
    "    --region us-central1 \\\n",
    "    --temp_location gs://jellyfish-training-demo-6/temp \\\n",
    "    --job_name \"pageview-counter-final-job-$(date +'%Y%m%d-%H%M%S')\" \\\n",
    "    --subscription projects/jellyfish-training-demo-6/subscriptions/dsl-clickstream-ddos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
