{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c37a40-7af1-444f-9979-6f1b08efacf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install necessary libraries\n",
    "# Run this cell if you haven't installed the BigQuery client library yet.\n",
    "!pip install google-cloud-bigquery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8d239-be07-4236-a493-259b597fb485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "from datetime import datetime\n",
    "from google.cloud import storage\n",
    "import os\n",
    "from google.api_core import exceptions # Import for catching Google API exceptions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50addd-745b-4af6-8089-fe15bedaccd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Configuration\n",
    "# IMPORTANT: Replace \"your-gcp-project-id\" with your actual Google Cloud Project ID.\n",
    "PROJECT_ID = \"jellyfish-training-demo-6\"\n",
    "DATASET_ID = \"dsl_project\"  # As specified in your BigQuery schema\n",
    "TABLE_ID = \"web_visits\"     # As specified in your BigQuery schema\n",
    "GCS_BUCKET_NAME = \"guillermo-lake\" # bucket name where jsonl files are stored\n",
    "\n",
    "# BigQuery Insert Batch Size\n",
    "# Adjust this value based on the average size of your rows.\n",
    "# A common starting point is 1000 or 10000 rows.\n",
    "BIGQUERY_INSERT_BATCH_SIZE = 5000\n",
    "\n",
    "# Path to your JSONL data file.\n",
    "# Make sure this file is accessible from your Jupyter Notebook environment.\n",
    "#storage_client = storage.Client(project=PROJECT_ID)\n",
    "##bucket = storage_client.get_bucket(BUCKET)\n",
    "#JSONL_FILE_PATH = \"visits_data.jsonl\"\n",
    "#GCS_FILE_PATH = \"\" #*.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99588264-f7c0-4f56-a3b6-9608b459a313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 4: BigQuery Client Initialization\n",
    "# Ensure you have authenticated to GCP before running this script.\n",
    "# (e.g., by setting GOOGLE_APPLICATION_CREDENTIALS env variable\n",
    "# or running `gcloud auth application-default login` in your terminal\n",
    "# where you launch Jupyter, or using `gcloud auth application-default login`\n",
    "# within a separate notebook cell if running on a GCP managed notebook)\n",
    "try:\n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    table_ref = client.dataset(DATASET_ID).table(TABLE_ID)\n",
    "    print(f\"BigQuery client initialized for project: {PROJECT_ID}\")\n",
    "    print(f\"Target table: {DATASET_ID}.{TABLE_ID}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing BigQuery client: {e}\")\n",
    "    print(\"Please ensure your Google Cloud credentials are set up correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d97b4a-0844-4ffd-878f-05dcc50937a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5: Function to transform a single JSON line to BigQuery row format\n",
    "def transform_json_to_bigquery_row(json_line: str) -> dict:\n",
    "    \"\"\"\n",
    "    Transforms a single JSON line from the input file into a dictionary\n",
    "    suitable for insertion into the BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        json_line (str): A single line of JSON string from the input file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary representing a single row in the BigQuery table.\n",
    "    \"\"\"\n",
    "    data = json.loads(json_line)\n",
    "\n",
    "    # --- Geolocation Transformation ---\n",
    "    # Input: \"latitude,longitude\" (e.g., \"31.764576,28.583238\")\n",
    "    # BigQuery GEOGRAPHY type expects WKT format: \"POINT(longitude latitude)\"\n",
    "    geolocation_wkt = None\n",
    "    if 'geolocation' in data and data['geolocation']:\n",
    "        try:\n",
    "            lat, lon = map(float, data['geolocation'].split(','))\n",
    "            geolocation_wkt = f\"POINT({lon} {lat})\"\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Invalid geolocation format '{data['geolocation']}', setting to NULL for this row.\")\n",
    "\n",
    "    # --- Determine visit_start_time and visit_end_time from events ---\n",
    "    # visit_start_time is the timestamp of the earliest event.\n",
    "    # visit_end_time is the timestamp of the latest event.\n",
    "    visit_start_time = None\n",
    "    visit_end_time = None\n",
    "    if data.get('events'):\n",
    "        # Filter out events with missing or invalid timestamps to avoid errors\n",
    "        valid_events = []\n",
    "        for e in data['events']:\n",
    "            if 'event' in e and 'timestamp' in e['event']:\n",
    "                try:\n",
    "                    datetime.fromisoformat(e['event']['timestamp']) # Validate format\n",
    "                    valid_events.append(e)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Invalid timestamp format '{e['event'].get('timestamp')}' for an event. Skipping this event for time calculation.\")\n",
    "\n",
    "        if valid_events:\n",
    "            # Sort events by timestamp to find the earliest and latest\n",
    "            sorted_events = sorted(\n",
    "                valid_events,\n",
    "                key=lambda x: datetime.fromisoformat(x['event']['timestamp'])\n",
    "            )\n",
    "            visit_start_time = sorted_events[0]['event']['timestamp']\n",
    "            visit_end_time = sorted_events[-1]['event']['timestamp']\n",
    "        else:\n",
    "            print(\"Warning: No valid event timestamps found to determine visit start/end times.\")\n",
    "\n",
    "\n",
    "    # --- Transform events array ---\n",
    "    transformed_events = []\n",
    "    for event_data in data.get('events', []):\n",
    "        event_details = event_data.get('event', {}) # Use .get() for safer access\n",
    "        event_type = event_details.get('event_type')\n",
    "        event_timestamp = event_details.get('timestamp')\n",
    "        details_payload = event_details.get('details', {})\n",
    "\n",
    "        # Initialize all possible event structs to None for BigQuery's NULL\n",
    "        page_view_struct = None\n",
    "        add_cart_struct = None\n",
    "        purchase_struct = None\n",
    "\n",
    "        # Populate the specific struct based on event_type\n",
    "        if event_type == 'page_view':\n",
    "            page_view_struct = {\n",
    "                'page_url': details_payload.get('page_url'),\n",
    "                'referrer_url': details_payload.get('referrer_url')\n",
    "            }\n",
    "        elif event_type == 'add_item_to_cart':\n",
    "            add_cart_struct = {\n",
    "                'product_id': details_payload.get('product_id'),\n",
    "                'product_name': details_payload.get('product_name'),\n",
    "                'category': details_payload.get('category'),\n",
    "                # Safely convert to float/int, default to 0.0/0 if missing or invalid\n",
    "                'price': float(details_payload.get('price', 0.0)) if details_payload.get('price') is not None else 0.0,\n",
    "                'quantity': int(details_payload.get('quantity', 0)) if details_payload.get('quantity') is not None else 0\n",
    "            }\n",
    "        elif event_type == 'purchase':\n",
    "            purchase_items = []\n",
    "            for item in details_payload.get('items', []):\n",
    "                purchase_items.append({\n",
    "                    'product_id': item.get('product_id'),\n",
    "                    'product_name': item.get('product_name'),\n",
    "                    'category': item.get('category'),\n",
    "                    'price': float(item.get('price', 0.0)) if item.get('price') is not None else 0.0,\n",
    "                    'quantity': int(item.get('quantity', 0)) if item.get('quantity') is not None else 0\n",
    "                })\n",
    "            purchase_struct = {\n",
    "                'order_id': details_payload.get('order_id'),\n",
    "                'amount': float(details_payload.get('amount', 0.0)) if details_payload.get('amount') is not None else 0.0,\n",
    "                'currency': details_payload.get('currency'),\n",
    "                'items': purchase_items\n",
    "            }\n",
    "\n",
    "        transformed_events.append({\n",
    "            'event_type': event_type,\n",
    "            'event_timestamp': event_timestamp,\n",
    "            'page_view': page_view_struct,\n",
    "            'add_cart': add_cart_struct,\n",
    "            'purchase': purchase_struct\n",
    "        })\n",
    "\n",
    "    # Construct the final row dictionary matching BigQuery schema\n",
    "    return {\n",
    "        'session_id': data.get('session_id'),\n",
    "        'user_id': data.get('user_id'),\n",
    "        'device_type': data.get('device_type'),\n",
    "        'geolocation': geolocation_wkt,\n",
    "        'user_agent': data.get('user_agent'),\n",
    "        'visit_start_time': visit_start_time,\n",
    "        'visit_end_time': visit_end_time,\n",
    "        'events': transformed_events\n",
    "    }\n",
    "\n",
    "\n",
    "# Cell 6: Main script execution function\n",
    "def load_data_to_bigquery():\n",
    "    \"\"\"\n",
    "    Reads all JSONL files from the specified Google Cloud Storage bucket's root,\n",
    "    transforms each line, and loads the transformed data into the\n",
    "    specified BigQuery table.\n",
    "    \"\"\"\n",
    "    all_rows_to_insert = [] # Collect all rows from all files\n",
    "    print(f\"Attempting to read all .jsonl files from gs://{GCS_BUCKET_NAME}...\")\n",
    "\n",
    "    try:\n",
    "        storage_client = storage.Client(project=PROJECT_ID)\n",
    "        bucket = storage_client.get_bucket(GCS_BUCKET_NAME)\n",
    "\n",
    "        # List all blobs (files) in the root of the bucket\n",
    "        blobs = bucket.list_blobs(prefix='')\n",
    "\n",
    "        processed_files_count = 0\n",
    "        for blob in blobs:\n",
    "            if blob.name.endswith('.jsonl'):\n",
    "                print(f\"  Processing file: {blob.name}\")\n",
    "                processed_files_count += 1\n",
    "                try:\n",
    "                    jsonl_content = blob.download_as_text()\n",
    "                    lines = jsonl_content.splitlines()\n",
    "\n",
    "                    for line_num, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                        try:\n",
    "                            row = transform_json_to_bigquery_row(line)\n",
    "                            all_rows_to_insert.append(row)\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"    Error decoding JSON on line {line_num + 1} in {blob.name}: {e}. Skipping line: {line[:100]}...\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"    An unexpected error occurred processing line {line_num + 1} in {blob.name}: {e}. Skipping line: {line[:100]}...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error downloading or processing file {blob.name}: {e}\")\n",
    "\n",
    "        if processed_files_count == 0:\n",
    "            print(f\"No .jsonl files found in the root of bucket '{GCS_BUCKET_NAME}'. Nothing to insert.\")\n",
    "            return\n",
    "\n",
    "        if not all_rows_to_insert:\n",
    "            print(\"No valid rows were extracted from the JSONL files. Nothing to insert.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Successfully processed {len(all_rows_to_insert)} rows from {processed_files_count} JSONL files.\")\n",
    "        print(f\"Attempting to insert {len(all_rows_to_insert)} rows into BigQuery table '{DATASET_ID}.{TABLE_ID}' in batches...\")\n",
    "\n",
    "        # --- Batching for BigQuery Inserts ---\n",
    "        total_inserted_rows = 0\n",
    "        for i in range(0, len(all_rows_to_insert), BIGQUERY_INSERT_BATCH_SIZE):\n",
    "            batch = all_rows_to_insert[i:i + BIGQUERY_INSERT_BATCH_SIZE]\n",
    "            print(f\"  Inserting batch {i // BIGQUERY_INSERT_BATCH_SIZE + 1} ({len(batch)} rows)...\")\n",
    "            try:\n",
    "                errors = client.insert_rows_json(table_ref, batch)\n",
    "                if errors:\n",
    "                    print(f\"    Encountered errors in batch {i // BIGQUERY_INSERT_BATCH_SIZE + 1}:\")\n",
    "                    for error in errors:\n",
    "                        print(f\"      Row index in batch: {error.get('index', 'N/A')}, Errors: {error.get('errors', 'N/A')}\")\n",
    "                else:\n",
    "                    print(f\"    Batch {i // BIGQUERY_INSERT_BATCH_SIZE + 1} successfully inserted.\")\n",
    "                    total_inserted_rows += len(batch)\n",
    "            except exceptions.GoogleAPICallError as e:\n",
    "                # Catching specific HTTP errors like 413, 500, etc.\n",
    "                print(f\"    Google API Call Error inserting batch {i // BIGQUERY_INSERT_BATCH_SIZE + 1}: {e}\")\n",
    "                print(\"    This batch might have failed. Please review the error details.\")\n",
    "                # You might want to add more robust error handling here, e.g.,\n",
    "                # logging failed rows for retry, or stopping if error is critical.\n",
    "            except Exception as e:\n",
    "                print(f\"    An unexpected error occurred during batch insertion: {e}\")\n",
    "\n",
    "        print(f\"\\nBigQuery data loading process completed. Total successfully inserted rows: {total_inserted_rows} into '{DATASET_ID}.{TABLE_ID}'.\")\n",
    "\n",
    "    except exceptions.NotFound as e: # Corrected exception for GCS bucket/file not found\n",
    "        print(f\"Error: Bucket '{GCS_BUCKET_NAME}' or an object within it not found. Please check the name and permissions. Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during GCS file processing or BigQuery operation: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea61d3a-6755-4404-9109-5c198265a685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 7: Execute the data loading process\n",
    "if __name__ == \"__main__\":\n",
    "    load_data_to_bigquery()\n",
    "    print(\"\\nBigQuery data loading process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e8a3c-7d74-4d32-9e71-614027e6a447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
