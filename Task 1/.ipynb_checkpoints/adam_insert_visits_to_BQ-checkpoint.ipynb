{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489209b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parse_visit function\n",
    "import sys\n",
    "lib_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'dsllib'))\n",
    "if lib_path not in sys.path:\n",
    "    print(f\"Appending path {lib_path}\")\n",
    "    sys.path.append(lib_path)\n",
    "\n",
    "from dsllib.visits import parse_visit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b03130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "credentials, project = google.auth.default()\n",
    "print(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d45292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "PROJECT_ID=os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
    "REGION=os.environ.get(\"REGION\")\n",
    "BUCKET=os.environ.get(\"BUCKET\")\n",
    "DATASET=os.environ.get(\"DATASET\")\n",
    "TABLE=os.environ.get(\"TABLE\")\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in the table schema\n",
    "with open('../dsllib/table_schema.json', 'r') as f:\n",
    "    bq_schema = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing date.\n",
    "#timestamp = \"2024-07-01T20:40:00\"\n",
    "#timestamp2 = datetime.fromisoformat(timestamp)\n",
    "#timestamp2 #Succeeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_visits(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(line)\n",
    "    \n",
    "    records = [parse_visit(x) for x in data]\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_visits(filename, bq_client, table):\n",
    "    records = load_visits(filename)\n",
    "\n",
    "    errors = bq_client.insert_rows(table, records)\n",
    "    if not errors:\n",
    "        print(f\"Inserted successfully from {filename}\")\n",
    "    else:\n",
    "        print(errors)\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data = load_visits('../challenge-clickstream/data/visits-2024-07-01.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the BigQuery client\n",
    "\n",
    "#should already be loaded at setup.\n",
    "#PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "#DATASET = os.environ.get(\"DATASET\")\n",
    "#TABLE = os.environ.get(\"TABLE\")\n",
    "#TABLE = \"web_visits\" #Hardcoded for testing\n",
    "\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "dataset_ref = bq_client.dataset(DATASET, project=PROJECT_ID)\n",
    "table_ref = dataset_ref.table(TABLE)\n",
    "table = bq_client.get_table(table_ref) \n",
    "\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b67b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_file = '../challenge-clickstream/data/visits-2024-07-02.jsonl'\n",
    "#errors = insert_visits(test_file, bq_client, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb57caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a local directory of files into BigQuery\n",
    "data_dir = '../challenge-clickstream/data'\n",
    "\n",
    "for filename in sorted(os.listdir(data_dir)):\n",
    "    if filename.endswith(\".jsonl\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        #errors = insert_visits(file_path, bq_client, table)\n",
    "        errors = None\n",
    "        if errors:\n",
    "            print(f\"Errors occurred while inserting data from {filename}: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac55d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_from_gcs(bucket_name: str, gcs_prefix: str, bq_client, table, project: str):\n",
    "    \"\"\"\n",
    "    Batch inserts JSONL files from a Cloud Storage bucket into a BigQuery table.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the Cloud Storage bucket.\n",
    "        gcs_prefix (str): The prefix for the JSONL files in the bucket (e.g., 'data/').\n",
    "        bq_client (google.cloud.bigquery.Client): The BigQuery client.\n",
    "        table (google.cloud.bigquery.Table): The BigQuery table object.\n",
    "        project (str): The Google Cloud project ID used for accessing resources.\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client(project=project)\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    for blob in bucket.list_blobs(prefix=gcs_prefix):\n",
    "        if blob.name.endswith(\".jsonl\"):\n",
    "            print(f\"Processing file: gs://{bucket_name}/{blob.name}\")\n",
    "            # Download the blob's content as a string\n",
    "            jsonl_string = blob.download_as_string().decode(\"utf-8\")\n",
    "\n",
    "            # Split the string into individual JSON lines\n",
    "            jsonl_lines = jsonl_string.splitlines()\n",
    "\n",
    "            # Parse each JSON line into a record\n",
    "            records = [parse_visit(line) for line in jsonl_lines if line.strip()]\n",
    "\n",
    "            # Insert the records into BigQuery\n",
    "            errors = bq_client.insert_rows(table, records)\n",
    "\n",
    "            if errors:\n",
    "                print(f\"Errors occurred while inserting data from gs://{bucket_name}/{blob.name}: {errors}\")\n",
    "            else:\n",
    "                print(f\"Inserted successfully from gs://{bucket_name}/{blob.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1caa768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert jsonl data from Cloud Storage to BigQuery.\n",
    "gcs_prefix = \"data/\"\n",
    "insert_from_gcs(BUCKET, gcs_prefix, bq_client, table, PROJECT_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Beam 2.63.0 (Local)",
   "language": "python",
   "name": "apache-beam-2.63.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
