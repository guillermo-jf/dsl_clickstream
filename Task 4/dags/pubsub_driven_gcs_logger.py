# pubsub_driven_gcs_logger_dag.py

from __future__ import annotations

import pendulum
import json

from airflow.models.dag import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.google.cloud.sensors.pubsub import PubSubPullSensor
from airflow.operators.python import PythonOperator

# --- CONFIGURATION ---
GCP_PROJECT_ID = "jellyfish-training-demo-6"  # Your GCP Project ID
PUBSUB_SUBSCRIPTION = "gcs-triggers-sub"     # A name for the subscription the sensor will create
PUBSUB_TOPIC = "gcs-clickstream-triggers"    # The topic you created in Step 1

# This Python function will be used to process the message from Pub/Sub
def process_gcs_notification(ti):
    """
    Pulls the message from the PubSub sensor, extracts the GCS file name,
    and pushes it to XCom for the next task.
    """
    # The sensor pushes the received messages to XCom
    message_list = ti.xcom_pull(task_ids="pull_gcs_notification", key="return_value")
    if not message_list:
        print("No new messages found.")
        return

    # In case of multiple messages, we'll just process the first one for this example
    message = message_list[0]
    
    # The message data is base64 encoded, but the sensor decodes it for us.
    # The Pub/Sub message payload from GCS is a JSON string.
    notification_str = message["message"]["data"]
    notification = json.loads(notification_str)

    file_name = notification.get("name")
    bucket_name = notification.get("bucket")
    
    print(f"Received notification for file: {file_name} in bucket: {bucket_name}")

    # Push the file name to XCom for the next task to use
    ti.xcom_push(key="file_name", value=file_name)


with DAG(
    dag_id="pubsub_driven_gcs_logger",
    start_date=pendulum.datetime(2023, 1, 1, tz="UTC"),
    # The DAG runs every minute to check for new messages.
    # Using a standard cron expression for "every minute".
    schedule="* * * * *",
    catchup=False,
    tags=["gcp", "gcs", "pubsub", "sensor"],
    doc_md="""
    ### Pub/Sub Driven GCS Logger

    This DAG uses a sensor to listen for messages on a Pub/Sub topic that are
    generated by GCS file uploads.
    """,
) as dag:
    # Task 1: The sensor. It pokes the Pub/Sub subscription for new messages.
    # The 'gcp_conn_id' must have permissions to pull from Pub/Sub.
    # The service account for your Composer environment typically has this by default.
    pull_gcs_notification = PubSubPullSensor(
        task_id="pull_gcs_notification",
        project_id=GCP_PROJECT_ID,
        subscription=PUBSUB_SUBSCRIPTION,
        gcp_conn_id="google_cloud_default",
        # The sensor will wait for a message and then succeed.
        # It will poke for 60 seconds, then defer until the next schedule interval.
        poke_interval=10,
        timeout=60,
    )

    # Task 2: Process the notification to extract the filename
    process_notification_task = PythonOperator(
        task_id="process_notification",
        python_callable=process_gcs_notification,
    )

    # Task 3: Log the filename that was extracted
    log_triggered_file = BashOperator(
        task_id="log_triggered_file",
        # Pull the file_name from the XCom that was pushed by the 'process_notification' task
        bash_command="echo 'Successfully processed trigger for file: {{ ti.xcom_pull(task_ids=\"process_notification\", key=\"file_name\") }}'",
    )

    # Define the task dependency chain
    pull_gcs_notification >> process_notification_task >> log_triggered_file
